{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for NLP\n",
    "Transfer learning is still in its nascent field so the field and open-source community has not settled entirely on one easy and bulletproof-solutions. This means that libraries are still being developed and are changing as we speak. However major hubs are beginning to form and conform its use. The first one is based on the tensorflow framework: `TensorflowHub`. The second one: `transformers` (formerly `pytorch-transformers`), came from PyTorch but adding tensorflow support also. \n",
    "\n",
    "For more classic work on Word-embeddings the gensim package which you worked with last week also has some decent ressorces. \n",
    "\n",
    "In this exercise set we will practice loading and applying models from both `tfhub` and from `transformers`. We will practice using sentence/paragraph embeddings as input to a clustering algorithm, and finally try out the transformers library for pretraining a language model from scratch. \n",
    "\n",
    "Again we will use the Toxicity dataset. See download instructions in [week 7 exercises](https://github.com/ulfaslak/sds_tddl_2020/blob/master/exercises/exercises(7)_Categorydev_Class.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "path2tox_data = '/home/snorre/Dropbox/Forskning/PhD/undervisning/train.csv'\n",
    "df = pd.read_csv(path2tox_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample data to allow faster prototyping\n",
    "# df = df.sample(5000) # simple solution\n",
    "# stratified solution where we subsample from each meta data column to get a higher variance.\n",
    "strat_sample_cols = df.columns[3:23]\n",
    "samples = []\n",
    "n = 300\n",
    "for col in strat_sample_cols:\n",
    "    binary = pd.DataFrame((df[col]>0.5).astype(int))\n",
    "    samples+=[j for _,j in binary.groupby(col).apply(lambda x: x.sample(min(len(x),n//2))).index]\n",
    "idx = list(set(samples))\n",
    "df = df.iloc[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.1:** *Clustering of Sentence Representations with tfhub and the Universal Sentence Encoder*\n",
    "TFhub allows you plug and play with fully implemented pipelines, including preprocessing and forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = df.comment_text.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a0cf47c39045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Universal sentence encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "#Universal sentence encoder\n",
    "\n",
    "from absl import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "# Reduce logging output.\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  embeddings = session.run(embed(sample_texts))\n",
    "\n",
    "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "    print(\"Message: {}\".format(messages[i]))\n",
    "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "    message_embedding_snippet = \", \".join(\n",
    "        (str(x) for x in message_embedding[:3]))\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (full)",
   "language": "python",
   "name": "env_full"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
