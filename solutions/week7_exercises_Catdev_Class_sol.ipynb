{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Development, Weak Supervision and CLassification\n",
    "Today we are gonna practice the Discovery and Exploration steps involved in a Content Analysis project. \n",
    "And finally we are gonna look at the Implementation of baseline text classifiers as described in the [\"baseline_classification.ipynb\"](https://github.com/ulfaslak/sds_tddl_2020/blob/master/baseline_classification.ipynb) notebook. \n",
    "\n",
    "Overall you will learn how to\n",
    "- setup a HSBM topic model within a docker environment (HSBM is unfortunatly hard to install)\n",
    "- Practice Computer Assisted Query Building - [\"Computer-Assisted Keyword and Document Set Discovery from Unstructured Text\"](https://gking.harvard.edu/publications/computer-assisted-keyword-and-document-set-discovery-fromunstructured-text) and [\"Building a Twitter opinion lexicon from automatically-annotated tweets\"](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwi4vabi56PoAhWEjqQKHUalAzEQFjAAegQIAxAB&url=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS095070511630106X&usg=AOvVaw11N9i8bUc2fwbbq9vLKsLY)\n",
    "- Practice Weak Supervision techniques combining lexical appraoches with NLP parsing systems (Stanfordnlp).\n",
    "\n",
    "\n",
    "Todays exercise will use the Kaggle Toxicity Classification dataset: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n",
    "- Follow the url. Sign in and download the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "path2tox_data = '/home/snorre/Dropbox/Forskning/PhD/undervisning/train.csv'\n",
    "df = pd.read_csv(path2tox_data)\n",
    "# subsample data to allow faster prototyping\n",
    "df = df.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440065</th>\n",
       "      <td>782588</td>\n",
       "      <td>0.7</td>\n",
       "      <td>I just don't get why soccer is so trendy in TO...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>158034</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533788</th>\n",
       "      <td>896321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Excellent idea for Liberals to  beg Artificial...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>164014</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880106</th>\n",
       "      <td>5197467</td>\n",
       "      <td>0.4</td>\n",
       "      <td>And, your dear leader is a perfect human being...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>330803</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4815</th>\n",
       "      <td>247452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>If most people understood how unstable much of...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42555</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814392</th>\n",
       "      <td>5117583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Waiting for it to come out that it was a regis...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>325931</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  target                                       comment_text  \\\n",
       "440065   782588     0.7  I just don't get why soccer is so trendy in TO...   \n",
       "533788   896321     0.0  Excellent idea for Liberals to  beg Artificial...   \n",
       "880106  5197467     0.4  And, your dear leader is a perfect human being...   \n",
       "4815     247452     0.0  If most people understood how unstable much of...   \n",
       "814392  5117583     0.0  Waiting for it to come out that it was a regis...   \n",
       "\n",
       "        severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "440065              0.0      0.1              0.1     0.6     0.0    NaN   \n",
       "533788              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "880106              0.0      0.0              0.0     0.4     0.0    0.0   \n",
       "4815                0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "814392              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "\n",
       "        atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "440065      NaN  ...      158034  approved      0    0    0      0         0   \n",
       "533788      NaN  ...      164014  approved      0    0    0      3         0   \n",
       "880106      0.0  ...      330803  approved      0    0    0      0         5   \n",
       "4815        0.0  ...       42555  approved      0    0    0      0         0   \n",
       "814392      NaN  ...      325931  rejected      0    0    0      0         0   \n",
       "\n",
       "        sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "440065              0.0                         0                        10  \n",
       "533788              0.0                         0                         4  \n",
       "880106              0.0                        10                        10  \n",
       "4815                0.0                         4                         4  \n",
       "814392              0.0                         0                         4  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up HSBM\n",
    "HSBM is based on the Network Package graph-tool which unfortunatly is notoriously hard to install on your ordinary laptops. \n",
    "For those of you using Linux, you can try installing on your own computer using similar commands as instructed in the Google Cloud example. \n",
    "\n",
    "Instead we have two possibilties: \n",
    "1. Create a local \"server\" on your own computer using Docker.\n",
    "2. Use a Google Cloud server as introduced in Week 1 (see exercise 1 on how to setup Google Cloud)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker solution\n",
    "#### First Install docker (https://docs.docker.com/install/)\n",
    "#### Enable file sharing.\n",
    "Docker runs an operating system in a closed environment, so you have to create a port to share files. \n",
    "\n",
    "Go to the Docker settings: Right click the docker icon in press settings.\n",
    "Press the /Ressources tab.\n",
    "Then /File sharing tab.\n",
    "Select the drive to be shared.\n",
    "\n",
    "### Setup the Graph-tool docker image used for HSBM.\n",
    "#### pull the image.\n",
    "`docker pull tiagopeixoto/graph-tool`\n",
    "\n",
    "#### Run the image while mounting the drive to be shared. # first allow shared drives in settings of the docker app\n",
    "#### run this.\n",
    "`docker run -v c:\\:/mnt/c -p 8888:8888 -p 6006:6006  --name graphtool -it -u root -w /home/root tiagopeixoto/graph-tool bash`\n",
    "##### In this example it was a c\\: directory that I shared. c:\\  specified the path to the shared directory on the host (your computer), and /mnt/c refered to the path in the Docker container.\n",
    " \n",
    "#### Run Jupyter notebook and navigate to port localhost://8888 in your browser\n",
    "`jupyter notebook --ip 0.0.0.0 --allow-root`\n",
    "\n",
    "#### When using it the second time use the following commands, to simple start and attach to the container.\n",
    "`docker container start graphtool`\n",
    "\n",
    "`docker attach graphtool`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Solution\n",
    "- Login to the server you set up in week 1. See these instructions for setting it up (https://course.fast.ai/start_gcp.html)\n",
    "- Run the following conda commands:\n",
    "    - ```conda config --add channels conda-forge\n",
    "conda config --add channels ostrokach-forge\n",
    "conda config --add channels pkgw-forge\n",
    "conda install gtk3 pygobject graph-tool cairo```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Running a HSBM topic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a New .ipynb and copy the following cell to download the TOPSBM tutorial created by the Authors of HSBM.\n",
    "import requests\n",
    "for filename in ['corpus.txt','titles.txt','sbmtm.py','TopSBM-tutorial.ipynb']:\n",
    "    url = 'https://raw.githubusercontent.com/martingerlach/hSBM_Topicmodel/master/%s'%filename\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "# Follow the Tutorial to test out the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.extra: Run the HSBM model on the Toxicity Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sbmtm\n",
    "# prepare documents\n",
    "import re\n",
    "url_re = re.compile('(?:https?:://)?www.[^ ]+')\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "def preprocess_doc(string,tokenizer=tokenizer.tokenize):\n",
    "    \"simple preprocessing function\"\n",
    "    doc = string.lower() # lowercase\n",
    "    doc = url_re.sub('url',doc)\n",
    "    doc = tokenizer(doc)\n",
    "    return doc\n",
    "\n",
    "df['doc'] = df.comment_text.apply(preprocess_doc).values\n",
    "docs = df['doc']\n",
    "# remove infrequent words.\n",
    "from collections import Counter\n",
    "cutoff = 5\n",
    "c = Counter()\n",
    "for doc in docs:\n",
    "    c.update(Counter(doc))\n",
    "vocab = c.most_common(25000)\n",
    "vocab = set([w for w,count in vocab if count>5])\n",
    "# remove words\n",
    "docs = [[w for w in doc if w in vocab] for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sbmtm.sbmtm()\n",
    "model.make_graph(docs,documents=['%d'%i for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/Dropbox/Undervisning/test_material/transferlearning/sbmtm.py:544: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p_td_d = (n_db/np.sum(n_db,axis=1)[:,np.newaxis]).T\n",
      "/home/snorre/Dropbox/Undervisning/test_material/transferlearning/sbmtm.py:550: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p_tw_d = (n_dbw/np.sum(n_dbw,axis=1)[:,np.newaxis]).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [('i', 0.6180613090306545),\n",
       "  ('my', 0.1164043082021541),\n",
       "  ('me', 0.07187241093620547),\n",
       "  (\"'m\", 0.048260149130074564),\n",
       "  ('am', 0.03707539353769677),\n",
       "  (\"'ve\", 0.03210439105219553),\n",
       "  (\"'ll\", 0.025890637945318973),\n",
       "  ('family', 0.025062137531068767),\n",
       "  ('guess', 0.01284175642087821),\n",
       "  ('seen', 0.012427506213753107)],\n",
       " 1: [(\"n't\", 0.041525254541824715),\n",
       "  ('do', 0.036309642643242164),\n",
       "  ('what', 0.027575364344180474),\n",
       "  ('about', 0.019739468955879417),\n",
       "  ('just', 0.01736873627470553),\n",
       "  ('like', 0.01736873627470553),\n",
       "  ('when', 0.016270712717109205),\n",
       "  ('out', 0.01584647634258335),\n",
       "  ('how', 0.014773407865841486),\n",
       "  ('did', 0.012752046316630065)],\n",
       " 2: [('for', 0.0878066964890877),\n",
       "  ('are', 0.07048935881794767),\n",
       "  ('have', 0.06096651755456148),\n",
       "  ('be', 0.06049206994713298),\n",
       "  ('they', 0.05761149518774569),\n",
       "  ('will', 0.03788803036464688),\n",
       "  ('we', 0.03768469567574895),\n",
       "  ('if', 0.03561745967195337),\n",
       "  ('would', 0.032499661108851836),\n",
       "  ('their', 0.03209299173105599)],\n",
       " 3: [('you', 0.3748344370860927),\n",
       "  ('?', 0.2937748344370861),\n",
       "  ('your', 0.1209271523178808),\n",
       "  ('why', 0.05509933774834437),\n",
       "  (\"'re\", 0.029403973509933776),\n",
       "  ('comment', 0.015099337748344372),\n",
       "  ('wrong', 0.014172185430463577),\n",
       "  ('comments', 0.013774834437086093),\n",
       "  ('question', 0.013509933774834438),\n",
       "  ('post', 0.011258278145695364)],\n",
       " 4: [('.', 0.20163550394504245),\n",
       "  (',', 0.13719262442333396),\n",
       "  ('to', 0.10402264986131271),\n",
       "  ('a', 0.0786134145815668),\n",
       "  ('is', 0.06060562509880571),\n",
       "  ('that', 0.054986274988861904),\n",
       "  ('it', 0.044063753036029954),\n",
       "  (\"'s\", 0.029447694054411406),\n",
       "  ('not', 0.028297954901481726),\n",
       "  ('this', 0.02441758526034406)],\n",
       " 5: [('game', 0.06632213608957795),\n",
       "  ('play', 0.04823428079242033),\n",
       "  ('team', 0.040482342807924204),\n",
       "  ('denver', 0.030146425495262703),\n",
       "  ('players', 0.02756244616709733),\n",
       "  ('season', 0.02756244616709733),\n",
       "  ('playing', 0.026701119724375538),\n",
       "  ('pick', 0.024117140396210164),\n",
       "  ('proud', 0.02239448751076658),\n",
       "  ('winning', 0.02239448751076658)],\n",
       " 6: [('the', 0.3316691369581611),\n",
       "  ('and', 0.1604389317939517),\n",
       "  ('of', 0.1470733546558494),\n",
       "  ('in', 0.10091790831318095),\n",
       "  ('as', 0.03978469459396209),\n",
       "  ('by', 0.02626309904568739),\n",
       "  ('from', 0.023532776867670385),\n",
       "  ('has', 0.02189458356086018),\n",
       "  ('there', 0.020828457758015447),\n",
       "  ('were', 0.013287567933016095)],\n",
       " 7: [('canada', 0.08030460366908965),\n",
       "  ('country', 0.06853582554517133),\n",
       "  ('canadian', 0.032883350640359986),\n",
       "  ('countries', 0.029075804776739357),\n",
       "  ('canadians', 0.024575977847005884),\n",
       "  ('millions', 0.021114572516441676),\n",
       "  ('china', 0.020076150917272412),\n",
       "  ('policy', 0.01938386985115957),\n",
       "  ('foreign', 0.017999307718933887),\n",
       "  ('members', 0.017307026652821047)],\n",
       " 8: [('vote', 0.04867090977162111),\n",
       "  ('party', 0.04679895170348184),\n",
       "  ('states', 0.038562336203669036),\n",
       "  ('liberals', 0.036690378135529764),\n",
       "  ('trudeau', 0.03444402845376263),\n",
       "  ('liberal', 0.03444402845376263),\n",
       "  ('election', 0.03369524522650692),\n",
       "  ('majority', 0.02995132909022838),\n",
       "  ('class', 0.023212280044926994),\n",
       "  ('national', 0.022463496817671284)],\n",
       " 9: [('school', 0.08384819064430715),\n",
       "  ('kids', 0.07237422771403354),\n",
       "  ('education', 0.05648720211827008),\n",
       "  ('water', 0.05472197705207414),\n",
       "  ('students', 0.04677846425419241),\n",
       "  ('schools', 0.04324801412180053),\n",
       "  ('parents', 0.03795233892321271),\n",
       "  ('teachers', 0.02559576345984113),\n",
       "  ('kid', 0.02383053839364519),\n",
       "  ('tv', 0.02118270079435128)],\n",
       " 10: [(')', 0.056333980024430554),\n",
       "  ('more', 0.05504059782999209),\n",
       "  ('(', 0.052884960839261336),\n",
       "  ('-', 0.03707695624056909),\n",
       "  ('than', 0.03434648271897679),\n",
       "  ('years', 0.025005389092476828),\n",
       "  ('government', 0.022490479269957606),\n",
       "  ('over', 0.022346770137242223),\n",
       "  ('its', 0.015808004598692246),\n",
       "  ('new', 0.014442767837896098)],\n",
       " 11: [('money', 0.05997109826589595),\n",
       "  ('tax', 0.05997109826589595),\n",
       "  ('pay', 0.05057803468208093),\n",
       "  ('care', 0.04238921001926782),\n",
       "  ('taxes', 0.02842003853564547),\n",
       "  ('health', 0.027215799614643547),\n",
       "  ('income', 0.02336223506743738),\n",
       "  ('jobs', 0.019990366088631986),\n",
       "  ('paid', 0.018304431599229287),\n",
       "  ('costs', 0.01758188824662813)],\n",
       " 12: [('trump', 0.21689425214353764),\n",
       "  ('president', 0.0619244204509368),\n",
       "  ('obama', 0.049539536360749446),\n",
       "  ('america', 0.039377580184185454),\n",
       "  ('clinton', 0.03016830739917434),\n",
       "  ('hillary', 0.02667513496348047),\n",
       "  ('campaign', 0.022864401397268974),\n",
       "  ('republicans', 0.022229279136233723),\n",
       "  ('republican', 0.020959034614163225),\n",
       "  ('donald', 0.0193712289615751)],\n",
       " 13: [('was', 0.19355783308931185),\n",
       "  ('had', 0.05300146412884334),\n",
       "  ('she', 0.045973645680819915),\n",
       "  ('her', 0.03616398243045388),\n",
       "  ('said', 0.031039531478770133),\n",
       "  ('after', 0.03074670571010249),\n",
       "  (';', 0.027964860907759882),\n",
       "  ('again', 0.025183016105417278),\n",
       "  ('â€™', 0.02474377745241581),\n",
       "  ('last', 0.023279648609077597)],\n",
       " 14: [('along', 0.046420141620771044),\n",
       "  ('air', 0.040912667191188044),\n",
       "  ('management', 0.022816679779701022),\n",
       "  ('areas', 0.022816679779701022),\n",
       "  ('near', 0.022816679779701022),\n",
       "  ('fish', 0.01966955153422502),\n",
       "  ('limited', 0.01966955153422502),\n",
       "  ('quality', 0.01888276947285602),\n",
       "  ('wild', 0.01730920535011802),\n",
       "  ('7', 0.015735641227380016)],\n",
       " 15: [('such', 0.02138380238417107),\n",
       "  ('life', 0.019417475728155338),\n",
       "  ('person', 0.019171684896153375),\n",
       "  ('must', 0.016836671992134695),\n",
       "  ('others', 0.015730613248125844),\n",
       "  ('issue', 0.013272704928106182),\n",
       "  ('however', 0.012658227848101266),\n",
       "  ('true', 0.010937692024087502),\n",
       "  ('issues', 0.010937692024087502),\n",
       "  ('perhaps', 0.010569005776084552)],\n",
       " 16: [('$', 0.07267605633802816),\n",
       "  ('%', 0.07014084507042254),\n",
       "  ('year', 0.048450704225352116),\n",
       "  ('million', 0.02647887323943662),\n",
       "  ('cost', 0.02619718309859155),\n",
       "  ('per', 0.021690140845070423),\n",
       "  ('dollars', 0.01971830985915493),\n",
       "  ('economy', 0.01887323943661972),\n",
       "  ('housing', 0.015211267605633802),\n",
       "  ('3', 0.014929577464788733)],\n",
       " 17: [('law', 0.08073878627968338),\n",
       "  ('police', 0.05910290237467018),\n",
       "  ('rights', 0.04432717678100264),\n",
       "  ('gun', 0.03852242744063324),\n",
       "  ('legal', 0.03799472295514512),\n",
       "  ('immigration', 0.03218997361477573),\n",
       "  ('laws', 0.027440633245382585),\n",
       "  ('actions', 0.026385224274406333),\n",
       "  ('behavior', 0.024802110817941952),\n",
       "  ('constitution', 0.024274406332453827)],\n",
       " 18: [('state', 0.13947633434038267),\n",
       "  ('oil', 0.06143001007049345),\n",
       "  ('alaska', 0.05387713997985901),\n",
       "  ('continue', 0.0377643504531722),\n",
       "  ('companies', 0.03323262839879154),\n",
       "  ('cut', 0.03272910372608258),\n",
       "  ('fund', 0.030211480362537766),\n",
       "  ('industry', 0.028700906344410877),\n",
       "  ('budget', 0.027190332326283987),\n",
       "  ('gas', 0.024672708962739175)],\n",
       " 19: [(\"''\", 0.5124766063630692), ('``', 0.48752339363693076)],\n",
       " 20: [('power', 0.07912457912457913),\n",
       "  ('bc', 0.04713804713804714),\n",
       "  ('risk', 0.04040404040404041),\n",
       "  ('ontario', 0.03535353535353535),\n",
       "  ('vancouver', 0.031144781144781145),\n",
       "  ('ndp', 0.030303030303030304),\n",
       "  ('build', 0.02861952861952862),\n",
       "  ('quebec', 0.026094276094276093),\n",
       "  ('basic', 0.025252525252525252),\n",
       "  ('create', 0.02356902356902357)],\n",
       " 21: [('white', 0.05583250249252243),\n",
       "  ('history', 0.0392156862745098),\n",
       "  ('american', 0.035559986706547024),\n",
       "  ('group', 0.03389830508474576),\n",
       "  ('war', 0.03256895978730475),\n",
       "  ('hate', 0.024260551678298437),\n",
       "  ('americans', 0.02326354270521768),\n",
       "  ('black', 0.022598870056497175),\n",
       "  ('racist', 0.02193419740777667),\n",
       "  ('north', 0.020272515785975408)],\n",
       " 22: [(':', 0.15998924442054316),\n",
       "  ('http', 0.021242269427265394),\n",
       "  ('u.s.', 0.02070449045442323),\n",
       "  ('https', 0.018553374563054583),\n",
       "  ('military', 0.01747781661737026),\n",
       "  ('problems', 0.015057811239580532),\n",
       "  ('act', 0.01452003226673837),\n",
       "  ('sexual', 0.012368916375369724),\n",
       "  ('false', 0.010217800484001076),\n",
       "  ('[', 0.009411132024737832)],\n",
       " 23: [('he', 0.4716535433070866),\n",
       "  ('his', 0.3),\n",
       "  ('him', 0.11692913385826771),\n",
       "  ('guy', 0.03858267716535433),\n",
       "  ('justice', 0.01811023622047244),\n",
       "  ('himself', 0.01732283464566929),\n",
       "  ('thinks', 0.013779527559055118),\n",
       "  ('father', 0.009448818897637795),\n",
       "  ('yesterday', 0.00826771653543307),\n",
       "  ('official', 0.005905511811023622)],\n",
       " 24: [('city', 0.06229036895064686),\n",
       "  ('leave', 0.03018687110685194),\n",
       "  ('hawaii', 0.029707714422616195),\n",
       "  ('rail', 0.028749401054144707),\n",
       "  ('local', 0.027791087685673215),\n",
       "  ('area', 0.026832774317201723),\n",
       "  ('land', 0.026832774317201723),\n",
       "  ('homeless', 0.023957834211787255),\n",
       "  ('drive', 0.020124580737901295),\n",
       "  ('eugene', 0.01964542405366555)],\n",
       " 25: [('man', 0.06517047906776004),\n",
       "  ('women', 0.06171773845489858),\n",
       "  ('god', 0.0487699611566681),\n",
       "  ('church', 0.04359085023737592),\n",
       "  ('catholic', 0.028053517479499353),\n",
       "  ('woman', 0.02762192490289167),\n",
       "  ('death', 0.02157962883038412),\n",
       "  ('jesus', 0.021148036253776436),\n",
       "  ('christian', 0.014242555028053518),\n",
       "  ('authority', 0.014242555028053518)],\n",
       " 26: [('...', 0.627906976744186),\n",
       "  ('please', 0.052183777651730004),\n",
       "  ('oh', 0.052183777651730004),\n",
       "  ('..', 0.03119682359614294),\n",
       "  ('#', 0.027226318774815655),\n",
       "  ('early', 0.01871809415768576),\n",
       "  ('nobody', 0.018150879183210438),\n",
       "  ('wow', 0.017016449234259785),\n",
       "  ('ha', 0.014180374361883154),\n",
       "  ('hey', 0.012478729438457176)],\n",
       " 27: [('!', 0.76656346749226),\n",
       "  ('--', 0.19628482972136224),\n",
       "  ('lol', 0.03715170278637771)],\n",
       " 28: [('information', 0.09136420525657071),\n",
       "  ('report', 0.06633291614518148),\n",
       "  ('investigation', 0.04755944931163955),\n",
       "  ('asking', 0.04630788485607009),\n",
       "  ('chief', 0.03879849812265332),\n",
       "  ('smart', 0.03504380475594493),\n",
       "  ('fbi', 0.03504380475594493),\n",
       "  ('fired', 0.028785982478097622),\n",
       "  ('officials', 0.028785982478097622),\n",
       "  ('details', 0.02753441802252816)]}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.2: Computer-Assisted Category Development\n",
    "First you need to install the [Gensim package](https://radimrehurek.com/gensim/)\n",
    "\n",
    "The package implements a range of unsupervised text methods, including classic topic modelling like LDA, Wordembeding models like Word2Vec and GloVe,  and an implementation of one of the early Paragraph Embedder \"Doc2Cec\". The package furthermore comes with a set of pretrained models that can be used for wordbased similarity search.\n",
    "\n",
    "In this exercise you will practice using pretrained Word Embeddings for similarity search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load a model from the Gensim (if not installed `! conda install -c anaconda gensim`) [pretrained models](https://github.com/RaRe-Technologies/gensim-data)\n",
    "    - I suggest you use the `\"glove-wiki-gigaword-300\"` model.\n",
    "\n",
    "```## Load gensim model\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")```\n",
    "\n",
    "- Play around with the `most_similar()` function of the model.\n",
    "    - Try defining both the `positive=` and `negative=` argument of the function. \n",
    "\n",
    "## Build a lexicon from scratch\n",
    "- Inspect 5-10 documents in the dataset to create an Initial list of positive and negative words, or come up with a topic yourself (e.g. politics).\n",
    "- Create an input function for evaluating new words.\n",
    "    - Use the model.get_similar(word) function to get candidate words.\n",
    "    - Run through the candidates.\n",
    "    - print the word to be evaluated.\n",
    "    - use the builtin method `input()` to get manual input.\n",
    "    - use the input to either save or discard candidate words.\n",
    "    - repeat process until you have >20 words.\n",
    "- Apply a lookup function to the dataset.\n",
    "    - First you tokenize the data using a tokenizer of choice. I suggest nltk.tokenize.casual.TweetTokenizer()\n",
    "    - create a function that loops through the words and counts matching with your lexicon.\n",
    "Extra: Train your own word2vec model on the Full toxicity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 14.3% 53.8/376.1MB downloaded"
     ]
    }
   ],
   "source": [
    "## Load gensim model\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('officers', 0.6411764621734619),\n",
       " ('policemen', 0.6110767722129822),\n",
       " ('arrested', 0.5419429540634155),\n",
       " ('policeman', 0.5184690952301025),\n",
       " ('authorities', 0.5138450264930725)]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(positive=['police','black'],negative=['white'])[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('officers', 0.6044589877128601),\n",
       " ('officials', 0.5710611343383789),\n",
       " ('authorities', 0.554427981376648),\n",
       " ('investigators', 0.5223734378814697),\n",
       " ('arrest', 0.5114766359329224)]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(positive=['police','white'],negative=['black'])[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = ['policy','trump','president','liberals','republican','democrats']\n",
    "lexicon = []\n",
    "done = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('policies', 0.8036329746246338),\n",
       " ('strategy', 0.6325587034225464),\n",
       " ('stance', 0.5960066318511963),\n",
       " ('administration', 0.5948634147644043),\n",
       " ('economic', 0.5877833366394043),\n",
       " ('reform', 0.5807735919952393),\n",
       " ('change', 0.578568160533905),\n",
       " ('decisions', 0.5783397555351257),\n",
       " ('monetary', 0.5625898241996765),\n",
       " ('agenda', 0.5607941746711731)]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar('policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ivana 0.49990522861480713\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "melania 0.45651504397392273\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "casino 0.45222747325897217\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nows 0.44631344079971313\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "knauss 0.436074823141098\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hilton 0.4234514832496643\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trumps 0.4143375754356384\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ivanka 0.40609344840049744\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "resorts 0.3992827236652374\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wynn 0.3902420997619629\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n",
      "Do you wish to continue? Press Enter else n y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vice 0.7302780747413635\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "presidents 0.6492104530334473\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "presidency 0.5860213041305542\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "presidential 0.5705326795578003\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bush 0.5581309795379639\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "clinton 0.5491981506347656\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "elect 0.5369746685028076\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "elected 0.525804877281189\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "w. 0.5164961814880371\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inauguration 0.4999786615371704\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n",
      "Do you wish to continue? Press Enter else n y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conservatives 0.8324412107467651\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "liberal 0.7376298904418945\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "democrats 0.678396463394165\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "socialists 0.6549149751663208\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conservative 0.6484119892120361\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "progressives 0.6397043466567993\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tories 0.6257392168045044\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reformers 0.6107094883918762\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "moderates 0.6034201979637146\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "republicans 0.5922455191612244\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Include in lexicon? PRess y else enter y\n",
      "Do you wish to continue? Press Enter else n n\n"
     ]
    }
   ],
   "source": [
    "while len(seeds)>0:\n",
    "    seed = seeds.pop(0)\n",
    "    if seed in glove.vocab:\n",
    "        similar = glove.most_similar(seed)\n",
    "    else:\n",
    "        done.add(seed)\n",
    "        lexicon.append(seed)\n",
    "        continue\n",
    "    for candidate,score in similar:\n",
    "        print()\n",
    "        print(candidate,score)\n",
    "        i = input('Include in lexicon? PRess y else enter')\n",
    "        if i=='y':\n",
    "            seeds.append(candidate)\n",
    "    if len(done)%10==0:\n",
    "        end = input('Do you wish to continue? Press Enter else n')\n",
    "        if end=='n':\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra train a word2vec model from scratch\n",
    "# run w2vec.\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "############\n",
    "## missing\n",
    "#####load pretrained and just retrain\n",
    "###############\n",
    "### Define parameters for the model ###\n",
    "size=300 # Size of Embedding.\n",
    "workers=4 # Number CPU Cores to use for training in Parallel.\n",
    "iter_= 50 # Depending on the size of your data you might want to run through your data more than once.\n",
    "window=6 # How much Context\n",
    "min_count=5 # Number of Occurrences to be kept in the Vocabulary\n",
    "### Initialize model and start training ###\n",
    "count = 0\n",
    "ws = Counter()\n",
    "for i in docs:\n",
    "    count+=len(i)\n",
    "    for w in i:\n",
    "        ws[w]+=1\n",
    "print('%d unique words in corpus and %d count'%(len(ws),count))\n",
    "max_words = 250000\n",
    "ws = dict(ws.most_common(max_words))\n",
    "min_words = 200000000\n",
    "\n",
    "iter_ = min_words//count\n",
    "iter_ = max(iter_,2)\n",
    "w2v = Word2Vec(size=size,workers=workers,negative=10,iter=0,window=window,min_count=min_count) # max_final_vocab = max_words\n",
    "w2v.build_vocab_from_freq(ws)\n",
    "\n",
    "print('Getting ready to train')\n",
    "print(w2v.corpus_count)\n",
    "for i in range(iter_):\n",
    "    w2v.train(docs,total_words = count,epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3: Weak supervision combining sentiment lexicon and dependency parser.\n",
    "This exercise we will create a weak supervision scheme, that combines the Vader sentiment analysis tool implemented in the nltk package, with a nlp parser to extract relationships - i.e. who is the negative sentiment directed against.\n",
    "\n",
    "First install the stanza package(`! pip install stanza`), formerly known as stanfordnlp.\n",
    "\n",
    "The package supports a [multitude of languages](https://stanfordnlp.github.io/stanza/models.html), but each model should be downloaded first using the `stanza.download()` function. Here we need to download the english one: `stanza.download('en')`.\n",
    "\n",
    "1.import nltk.sentiment and initialize the vader sentiment analyzer.\n",
    "\n",
    "2. Apply the vader sentiment analyzer extracting only the 'neg' value, and plot the results in relation to the columns: `['black','female','asian','homosexual_gay_or_lesbian']` as a barchart. The columns express the social groups involved/addressed in the comment. \n",
    "\n",
    "3. Make a column in the dataset that expresses whether 'woman' + relevant synonyms found using the Glove model, is mentioned in the text.\n",
    "\n",
    "4. Compute the precision and recall of the woman identifier in relation to the 'female' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.sentiment\n",
    "vader = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader_sentiment'] = df.comment_text.apply(lambda x: vader.polarity_scores(x)['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for col in ['black','female','asian','homosexual_gay_or_lesbian']:\n",
    "    y.append(df[df[col]>0]['vader_sentiment'].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.barh(['black','female','asian','homosexual_gay_or_lesbian'],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.7296419143676758), ('man', 0.6998662948608398), ('mother', 0.689943790435791), ('she', 0.6433226466178894), ('her', 0.6327143311500549), ('female', 0.6251603960990906), ('herself', 0.6215280294418335), ('person', 0.6170896887779236), ('women', 0.604761004447937), ('wife', 0.5986992120742798), ('daughter', 0.5714551210403442), ('pregnant', 0.5604141354560852), ('victim', 0.5527148246765137), ('husband', 0.5462620258331299), ('boy', 0.5453975200653076)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar('woman',topn=15)) # glove\n",
    "women_s = set(['woman','girl','mother','she','her','women','herself','wife','daughter'])\n",
    "\n",
    "df['women_match'] = df.doc.apply(lambda x: len(set(x)&women_s)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3927710843373494, 0.7990196078431373)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "sklearn.metrics.recall_score(df.women_match.astype(int),(df.female>0).astype(int)),sklearn.metrics.precision_score(df.women_match.astype(int),(df.female>0).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanfordnlp to match adjectives and women.\n",
    "\n",
    "5. Run the nlp pipeline by initializing the parser `stanza.Pipeline('en').\n",
    "\n",
    "6. Transform the Parse object to a list of sentences using the function `nlp_to_dict`.\n",
    "    - This will return a list of sentences, which by themselves are lsit of words including the parsed properties such as Word class.  \n",
    "\n",
    "7. Now we should get acquinted with the information involved. Count the most common lemmas of each wordclass(i.e. the 'upos' property), and print the top 10 words.\n",
    "\n",
    "8. Apply the `extract_adj_noun()` that returns a dataframe of noun-adjective pairs. \n",
    "\n",
    "9. Keep only rows where \"women\" + synonmyms is found in the noun column.\n",
    "\n",
    "10. Inspect the adjectives used, and compute which are significantly more common using the *Chi Squared measure*:\n",
    "\n",
    "$Chi^2$ express the difference between the co-occurence we observe and what we would have expected to see if two events independent, relative to the latter. I.e. how many times more prevalent is the co-occurrence we observe than what we would expect if they were independent.\n",
    "\n",
    "$X^{2}=\\sum_{i, j} \\frac{\\left(O_{i j}-E_{i j}\\right)^{2}}{E_{i j}}$\n",
    "where $i$ ranges over rows of the table, $j$ ranges over columns, $O_{i j}$ is the observed value for cell ($i,j$) and $E_ij$ is the expected value.\n",
    " \n",
    "\n",
    "11. Match the adjectives to two positive and negative word lists precompilled in the NLTK package. \n",
    "```\n",
    "from nltk.corpus import opinion_lexicon\n",
    "positive_w = set(opinion_lexicon.positive())\n",
    "negative_w = set(opinion_lexicon.negative())\n",
    "```\n",
    "12. Finally wrap this into a function, that returns true if both women+synonyms is present, and a negative word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 18.5MB/s]                    \n",
      "2020-03-18 20:06:15 INFO: Downloading default packages for language: en (English)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/en/default.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 402M/402M [00:52<00:00, 7.61MB/s] \n",
      "2020-03-18 20:07:16 INFO: Finished downloading models and saved to /home/snorre/stanza_resources.\n",
      "2020-03-18 20:07:16 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-03-18 20:07:17 INFO: Use device: cpu\n",
      "2020-03-18 20:07:17 INFO: Loading: tokenize\n",
      "2020-03-18 20:07:17 INFO: Loading: pos\n",
      "2020-03-18 20:07:18 INFO: Loading: lemma\n",
      "2020-03-18 20:07:18 INFO: Loading: depparse\n",
      "2020-03-18 20:07:19 INFO: Loading: ner\n",
      "2020-03-18 20:07:19 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nlp'] = df.comment_text.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_to_dict(doc,keep_sentence=True):\n",
    "    dependencies = []\n",
    "\n",
    "    if keep_sentence == True:\n",
    "        for sent in doc.sentences:\n",
    "            #keys = [i for i in dir(sent.words[0]) if '_' !=i[0] and i!='misc']\n",
    "            temp = []\n",
    "            for word in sent.words:\n",
    "                d = word.to_dict()\n",
    "                #d = {key:getattr(word,key) for key in keys}\n",
    "                d['index'] = int(d['id'])\n",
    "                temp.append(d)\n",
    "            dependencies.append(temp)\n",
    "    else:\n",
    "        max_id = 0\n",
    "        for sent in doc.sentences:\n",
    "            keys = [i for i in dir(sent.words[0]) if '_' !=i[0] and i!='misc']\n",
    "            words = sent.words\n",
    "            \n",
    "            for word in words:\n",
    "                d = word.to_dict()\n",
    "                #d = {key:getattr(word,key) for keyn in keys}\n",
    "                d['index'] = int(d['id'])\n",
    "                dependencies.append(d)\n",
    "    return dependencies\n",
    "df['nlp_parse'] = df.nlp.apply(nlp_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ2common = {}\n",
    "for d in df.nlp_parse.values:\n",
    "    \n",
    "\n",
    "    for l in d:\n",
    "        for i in l: \n",
    "            typ=i['upos']\n",
    "            if not typ in typ2common:\n",
    "                typ2common[typ] = Counter()\n",
    "            \n",
    "            c = typ2common[typ]\n",
    "            c[i['lemma']]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN [('people', 879), ('year', 545), ('time', 509), ('tax', 361), ('way', 346), ('government', 344), ('thing', 302), ('state', 285), ('country', 284), ('money', 256)]\n",
      "PUNCT [('.', 13996), (',', 9549), ('\"', 3423), ('?', 1936), ('-', 1450), (')', 753), ('(', 730), ('!', 678), ('...', 599), (\"'\", 496)]\n",
      "VERB [('have', 1659), ('be', 1028), ('do', 907), ('get', 716), ('make', 711), ('go', 679), ('say', 638), ('think', 567), ('know', 511), ('see', 489)]\n",
      "PRON [('you', 3758), ('I', 3319), ('they', 3201), ('it', 3160), ('he', 2275), ('we', 1978), ('that', 1799), ('who', 962), ('what', 927), ('this', 770)]\n",
      "ADP [('of', 5293), ('in', 3745), ('for', 2362), ('to', 2252), ('with', 1615), ('on', 1505), ('by', 869), ('from', 853), ('as', 710), ('at', 674)]\n",
      "DET [('the', 12788), ('a', 6254), ('this', 934), ('all', 920), ('no', 683), ('any', 469), ('some', 452), ('that', 407), ('these', 311), ('those', 227)]\n",
      "ADJ [('more', 474), ('many', 432), ('other', 430), ('good', 401), ('same', 241), ('own', 193), ('most', 185), ('better', 177), ('much', 170), ('new', 165)]\n",
      "AUX [('be', 11550), ('do', 1840), ('have', 1604), ('will', 1344), ('would', 1055), ('can', 923), ('should', 529), ('could', 302), ('may', 206), ('must', 138)]\n",
      "ADV [('so', 807), ('just', 695), ('when', 652), ('how', 594), ('not', 427), ('why', 416), ('now', 413), ('then', 403), ('even', 362), ('only', 341)]\n",
      "PROPN [('Trump', 651), ('Canada', 228), ('US', 184), ('Obama', 154), ('America', 122), ('President', 116), ('Alaska', 105), ('Clinton', 94), ('Trudeau', 93), ('God', 91)]\n",
      "PART [('to', 4924), ('not', 3283), (\"'s\", 1020), ('ay', 1), ('tmt', 1), (',t', 1), ('ha', 1), ('n;t', 1), ('o', 1), ('jus', 1)]\n",
      "CCONJ [('and', 6209), ('or', 1130), ('but', 1090), ('&', 140), ('either', 57), ('yet', 44), ('nor', 44), ('both', 35), ('plus', 26), ('neither', 8)]\n",
      "SCONJ [('that', 1626), ('if', 1054), ('as', 557), ('because', 351), ('of', 273), ('for', 232), ('while', 167), ('like', 150), ('by', 142), ('than', 129)]\n",
      "NUM [('one', 475), ('two', 168), ('million', 96), ('2', 88), ('1', 75), ('3', 66), ('10', 59), ('4', 56), ('50', 52), ('5', 50)]\n",
      "INTJ [('yes', 144), ('no', 112), ('well', 103), ('oh', 93), ('please', 90), ('lol', 65), ('yeah', 57), ('wow', 30), ('sorry', 24), ('hey', 23)]\n",
      "SYM [('%', 236), ('$', 221), ('-', 57), ('#', 32), ('+', 29), (';)', 9), (':)', 9), ('x', 7), ('/', 5), (':-)', 5)]\n",
      "X [('etc', 65), ('etc.', 31), ('2', 28), ('1', 27), ('3', 15), ('non', 12), ('i.e.', 11), ('s', 11), ('mid', 10), ('e.g.', 9)]\n"
     ]
    }
   ],
   "source": [
    "sort_types = sorted(typ2common,key=lambda x:sum(typ2common[x].values()),reverse=True )\n",
    "for typ in sort_types:\n",
    "    c = typ2common[typ]\n",
    "    print(typ,c.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adj(d):\n",
    "    \n",
    "    i2d = {}#i['id']:i for i in d}\n",
    "    for l in d:\n",
    "        for i in l:\n",
    "            i2d[i['id']] = i\n",
    "    pairs = []\n",
    "    for l in d:\n",
    "        for i in l:\n",
    "            if i['upos'] == 'ADJ':\n",
    "                head = i['head']\n",
    "                if head==0:\n",
    "                    continue\n",
    "                pairs.append((i,i2d[str(head)]))\n",
    "    dat = []\n",
    "    for i,j in pairs:\n",
    "        d = {}\n",
    "        for col in ['text','lemma']:\n",
    "            d['adj_%s'%col] = i[col]\n",
    "            d['obj_%s'%col] = j[col]\n",
    "            \n",
    "        dat.append(d)\n",
    "    return pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def get_subject_verb_object(doc):\n",
    "    \"Input should be a dependency parsed document\"\n",
    "    keep_types = set(['VERB','NOUN','ADJ','PROPN','ADV'])\n",
    "    relations = []\n",
    "\n",
    "    for num,sent in enumerate(doc):\n",
    "        subjects = []\n",
    "        objects = []\n",
    "        subject_verb_object = nx.DiGraph()\n",
    "        for w in sent:\n",
    "            typ = w['upos']\n",
    "            if not typ in keep_types:\n",
    "                continue\n",
    "            meta = {key:w[key] for key in ['upos','head','index','lemma','text','deprel']}\n",
    "            parent = w['head']\n",
    "            i = w['index']\n",
    "            rel = w['deprel']\n",
    "            if 'obl' in rel:\n",
    "                objects.append(i)\n",
    "            if 'nsubj' in rel:\n",
    "                subjects.append(i)\n",
    "            lemma = w['lemma']\n",
    "            text = w['text']\n",
    "            subject_verb_object.add_node(i,**meta)\n",
    "            subject_verb_object.node[i]['index'] = i\n",
    "            if parent!=0:\n",
    "                subject_verb_object.add_edge(i,parent)\n",
    "        # parse graph. \n",
    "\n",
    "        for i in subjects:\n",
    "            neighbors = subject_verb_object[i]\n",
    "            subj_d = subject_verb_object.node[i]\n",
    "            subj_meta = {'subj_'+i:subj_d[i] for i in ['index','lemma','text','deprel']}\n",
    "            pairs = []\n",
    "            for n in neighbors:\n",
    "\n",
    "                verb_d = subject_verb_object.node[n]\n",
    "                if len(verb_d)==0:\n",
    "                    continue\n",
    "                \n",
    "                verb_meta = {'verb_'+i:verb_d[i] for i in ['index','lemma','text','deprel']}\n",
    "                \n",
    "                verb_meta['ADJ'] = []\n",
    "                verb_meta['ADV'] = []\n",
    "                for n1,_ in subject_verb_object.in_edges(n):\n",
    "                    d = subject_verb_object.node[n1]\n",
    "                    rel = d['deprel']\n",
    "                    if 'obl' in rel:\n",
    "                        obl_meta = {'obl_'+i:d[i] for i in ['index','lemma','text','deprel']}\n",
    "                        verb_meta.update(obl_meta)\n",
    "                    else:\n",
    "                        typ = d['upos']\n",
    "                        if typ in verb_meta:\n",
    "                            verb_meta[typ].append({i:d[i] for i in ['index','lemma','text','deprel']})\n",
    "                d = subj_meta.copy()\n",
    "                d.update(verb_meta)\n",
    "                d['sent_num'] = num\n",
    "                relations.append(d)\n",
    "    return pd.DataFrame(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nlp_to_dict(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(d):\n",
    "    i2d = {}\n",
    "    g = nx.Graph()\n",
    "    for num,l in enumerate(d): \n",
    "        for i in l:\n",
    "            id_ = '%d_%d'%(num,int(i['id']))\n",
    "            i2d[id_] = i\n",
    "            parent = i['head'] \n",
    "            \n",
    "            if parent==0:\n",
    "                continue\n",
    "            parent = '%d_%d'%(num,parent)\n",
    "            g.add_edge(id_,parent)\n",
    "    # make full network\n",
    "    for i in g:\n",
    "        for j in g[i]:\n",
    "            for k in g[i]:\n",
    "                g.add_edge(j,k)\n",
    "    return g,i2d\n",
    "def extract_adj_noun(d):\n",
    "    g,i2d = make_network(d)\n",
    "    temp = []\n",
    "    for i in i2d:\n",
    "        d = i2d[i]\n",
    "        type = d['upos']\n",
    "        text_a,lemma_a = d['text'],d['lemma']\n",
    "        if type=='ADJ':\n",
    "            if not i in g:\n",
    "                continue\n",
    "            for n in g[i]:\n",
    "                if i2d[n]['upos'] == 'NOUN':\n",
    "                    d = i2d[n]\n",
    "                    text,lemma = d['text'],d['lemma']\n",
    "                    temp.append({'adj_text':text_a,'adj_lemma':lemma_a,'noun_text':text,'noun_lemma':lemma})\n",
    "    return pd.DataFrame(temp)\n",
    "adjnoun_df = extract_adj_noun(sample.nlp_parse.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for doc in df.nlp_parse:\n",
    "    adjnoun_df = extract_adj_noun(doc)\n",
    "    temp.append(adjnoun_df)\n",
    "adjnoun_df = pd.concat(temp,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjnoun_df['woman'] = adjnoun_df.noun_text.isin(women_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_adj = adjnoun_df[adjnoun_df.noun_text.isin(women_s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = adjnoun_df.groupby('woman')['adj_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = adjnoun_df.adj_text.value_counts()\n",
    "N = all_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = counts[False]\n",
    "pos = counts[True]\n",
    "perc = sum_pos/N\n",
    "exp = all_counts*perc\n",
    "obs = pos[all_counts.index].fillna(0)\n",
    "chi = ((obs-exp)**2)/exp\n",
    "## get direction\n",
    "sign = obs-exp\n",
    "chi_sign = chi*sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good     -10.288531\n",
       "public   -10.157001\n",
       "social    -6.689322\n",
       "free      -6.061369\n",
       "legal     -4.108758\n",
       "Name: adj_text, dtype: float64"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_sign.sort_values().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accountable    371.710371\n",
       "feminist       381.770324\n",
       "sexual         397.264136\n",
       "white          455.760422\n",
       "young          986.174920\n",
       "Name: adj_text, dtype: float64"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_sign.sort_values().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_lemma</th>\n",
       "      <th>adj_text</th>\n",
       "      <th>noun_lemma</th>\n",
       "      <th>noun_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>disgusting</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>afraid</td>\n",
       "      <td>afraid</td>\n",
       "      <td>girl</td>\n",
       "      <td>girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>irrelevant</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>partisan</td>\n",
       "      <td>partisan</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>negligent</td>\n",
       "      <td>negligent</td>\n",
       "      <td>wife</td>\n",
       "      <td>wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>authoritarian</td>\n",
       "      <td>authoritarian</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>authoritarian</td>\n",
       "      <td>authoritarian</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>lame</td>\n",
       "      <td>lame</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>ridiculous</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>primitive</td>\n",
       "      <td>primitive</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>murderous</td>\n",
       "      <td>murderous</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>conservative</td>\n",
       "      <td>conservative</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>unwanted</td>\n",
       "      <td>unwanted</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lazy</td>\n",
       "      <td>lazy</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>difficult</td>\n",
       "      <td>difficult</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dumb</td>\n",
       "      <td>dumb</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dumb</td>\n",
       "      <td>dumb</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scared</td>\n",
       "      <td>scared</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>helpless</td>\n",
       "      <td>helpless</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>irrelevant</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>daughter</td>\n",
       "      <td>daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>useless</td>\n",
       "      <td>useless</td>\n",
       "      <td>wife</td>\n",
       "      <td>wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>unbelievable</td>\n",
       "      <td>unbelievable</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>incorrigible</td>\n",
       "      <td>incorrigible</td>\n",
       "      <td>daughter</td>\n",
       "      <td>daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>violent</td>\n",
       "      <td>violent</td>\n",
       "      <td>daughter</td>\n",
       "      <td>daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>evil</td>\n",
       "      <td>evil</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>invisible</td>\n",
       "      <td>invisible</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tragic</td>\n",
       "      <td>tragic</td>\n",
       "      <td>girl</td>\n",
       "      <td>girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tragic</td>\n",
       "      <td>tragic</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>disproportionate</td>\n",
       "      <td>disproportionate</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>confused</td>\n",
       "      <td>confused</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>confused</td>\n",
       "      <td>confused</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>ridiculous</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>uncomfortable</td>\n",
       "      <td>uncomfortable</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stressful</td>\n",
       "      <td>stressful</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hazardous</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>wrong</td>\n",
       "      <td>wrong</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>inept</td>\n",
       "      <td>inept</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>criminal</td>\n",
       "      <td>criminal</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>critical</td>\n",
       "      <td>critical</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>illegal</td>\n",
       "      <td>illegal</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aggressive</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>annoying</td>\n",
       "      <td>annoying</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>annoying</td>\n",
       "      <td>annoying</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rude</td>\n",
       "      <td>rude</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>racist</td>\n",
       "      <td>racist</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>upset</td>\n",
       "      <td>upset</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unwanted</td>\n",
       "      <td>unwanted</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "      <td>girl</td>\n",
       "      <td>girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>greedy</td>\n",
       "      <td>greedy</td>\n",
       "      <td>girl</td>\n",
       "      <td>girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loud</td>\n",
       "      <td>loud</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>dangerous</td>\n",
       "      <td>dangerous</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>shameful</td>\n",
       "      <td>shameful</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unhappy</td>\n",
       "      <td>unhappy</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>hard</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tired</td>\n",
       "      <td>tired</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tired</td>\n",
       "      <td>tired</td>\n",
       "      <td>woman</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>wack</td>\n",
       "      <td>wack</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            adj_lemma          adj_text noun_lemma noun_text\n",
       "8          disgusting        disgusting      woman     women\n",
       "46             afraid            afraid       girl      girl\n",
       "1          irrelevant        irrelevant      woman     women\n",
       "19           partisan          partisan      woman     women\n",
       "18          negligent         negligent       wife      wife\n",
       "82      authoritarian     authoritarian      woman     women\n",
       "87      authoritarian     authoritarian      woman     women\n",
       "91               lame              lame      woman     woman\n",
       "84         ridiculous        ridiculous      woman     women\n",
       "14          primitive         primitive      woman     women\n",
       "17          murderous         murderous      woman     women\n",
       "9        conservative      conservative      woman     women\n",
       "14           unwanted          unwanted      woman     women\n",
       "32               lazy              lazy      woman     woman\n",
       "37          difficult         difficult      woman     woman\n",
       "3                dumb              dumb      woman     woman\n",
       "11               dumb              dumb      woman     woman\n",
       "0              scared            scared      woman     woman\n",
       "3            helpless          helpless      woman     woman\n",
       "45         irrelevant        irrelevant   daughter  daughter\n",
       "67            useless           useless       wife      wife\n",
       "16       unbelievable      unbelievable      woman     woman\n",
       "22       incorrigible      incorrigible   daughter  daughter\n",
       "33            violent           violent   daughter  daughter\n",
       "12               evil              evil      woman     women\n",
       "0           invisible         invisible      woman     women\n",
       "82              funny             funny     mother    mother\n",
       "3              tragic            tragic       girl      girl\n",
       "8              tragic            tragic      woman     woman\n",
       "9    disproportionate  disproportionate      woman     women\n",
       "..                ...               ...        ...       ...\n",
       "59           confused          confused      woman     woman\n",
       "75           confused          confused      woman     woman\n",
       "22         ridiculous        ridiculous      woman     women\n",
       "35                sad               sad      woman     woman\n",
       "11      uncomfortable     uncomfortable      woman     women\n",
       "20          stressful         stressful      woman     women\n",
       "29          hazardous         hazardous      woman     women\n",
       "54              wrong             wrong      woman     woman\n",
       "26              inept             inept     mother    mother\n",
       "37           criminal          criminal     mother    mother\n",
       "139          critical          critical      woman     women\n",
       "152           illegal           illegal      woman     women\n",
       "1                 bad               bad      woman     woman\n",
       "1          aggressive        aggressive      woman     women\n",
       "32           annoying          annoying      woman     women\n",
       "34           annoying          annoying      woman     women\n",
       "3                rude              rude      woman     woman\n",
       "5              racist            racist      woman     woman\n",
       "3               upset             upset      woman     women\n",
       "1            unwanted          unwanted      woman     women\n",
       "7                 bad               bad       girl      girl\n",
       "9              greedy            greedy       girl      girl\n",
       "2                loud              loud     mother    mother\n",
       "29          dangerous         dangerous      woman     woman\n",
       "34           shameful          shameful      woman     woman\n",
       "1             unhappy           unhappy      woman     women\n",
       "4                hard              hard      woman     woman\n",
       "8               tired             tired      woman     women\n",
       "48              tired             tired      woman     women\n",
       "30               wack              wack      woman     woman\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "positive_w = set(opinion_lexicon.positive())\n",
    "negative_w = set(opinion_lexicon.negative())\n",
    "women_adj[women_adj.adj_text.isin(negative_w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woman_negative(row):\n",
    "    nlp_parse = row['nlp_parse']\n",
    "    doc = row['doc']\n",
    "    women_in = len(set(doc)&women_s)>0\n",
    "    if not women_in:\n",
    "        return 0\n",
    "    adjnoun_df = extract_adj_noun(nlp_parse)\n",
    "    if len(adjnoun_df)==0:\n",
    "        return 0\n",
    "    \n",
    "    women_adj = adjnoun_df[adjnoun_df.noun_text.isin(women_s)]\n",
    "    if len(women_adj)==0:\n",
    "        return 0\n",
    "    score = len(women_adj[women_adj.adj_text.isin(negative_w)])\n",
    "    return score\n",
    "df['women_negative'] = df.apply(woman_negative,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "women_negative\n",
       "0    0.016757\n",
       "1    0.163716\n",
       "2    0.173611\n",
       "3    0.150000\n",
       "4    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('women_negative').apply(lambda x: (x.female*x.target).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 Compiling the lexicons. \n",
    "- Here you should follow the instructions from the lexical_methods.ipynb notebook of compilling and downloading the lexicons. \n",
    "- Alternatively you can download the precompilled lexicon functions that the notebook builds using the link supplied on absalon. \n",
    "- download the lexical_mining.py script that wraps around all individual lexicon functions.\n",
    "- import and appy the lexical_mining script using the `.lexical_mining()` function to the toxicity_dataset.\n",
    "- Compare the different variables in relation to the Groups: `['black','female','asian','homosexual_gay_or_lesbian']` and see if the different sentiment analytical methods agree on which group is recieving most hostility.\n",
    "    - Do this by computing the mean of the different sentiment variables: `['vader_neg','afinn_afinn','liu_negative_count','hedometer_happiness']` in relation to each group. \n",
    "    - Then compare the ranking made by each sentiment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexical_mining as lex\n",
    "lex_df = df.comment_text.apply(lex.lexical_mining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['black','female','asian','homosexual_gay_or_lesbian','conglomerate_sentiment_negative']:\n",
    "    lex_df[col] = df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cols = ['vader_neg','afinn_afinn','liu_negative_count','hedometer_happiness']\n",
    "temp = []\n",
    "\n",
    "    \n",
    "for meta_col in ['black','female','asian','homosexual_gay_or_lesbian']:\n",
    "    d = lex_df[lex_df[meta_col]>0].mean()[neg_cols]\n",
    "    d = pd.DataFrame(d).T\n",
    "    d.index = [meta_col]\n",
    "    \n",
    "    temp.append(d)\n",
    "    \n",
    "    \n",
    "sent_df = pd.concat(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vader_neg Index(['homosexual_gay_or_lesbian', 'female', 'asian', 'black'], dtype='object')\n",
      "afinn_afinn Index(['black', 'asian', 'homosexual_gay_or_lesbian', 'female'], dtype='object')\n",
      "liu_negative_count Index(['homosexual_gay_or_lesbian', 'female', 'asian', 'black'], dtype='object')\n",
      "hedometer_happiness Index(['black', 'asian', 'homosexual_gay_or_lesbian', 'female'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for col in sent_df.columns:\n",
    "    print(col,sent_df.sort_values(col).index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graphtool)",
   "language": "python",
   "name": "graphtool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
