{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " week8_exercises_transferlearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7vHMxOJJGcr",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning for NLP\n",
        "Transfer learning is still in its nascent field so the field and open-source community has not settled entirely on one easy and bulletproof-solutions. This means that libraries are still being developed and are changing as we speak. However major hubs are beginning to form and conform its use. The first one is based on the tensorflow framework: `tensorflow_hub`. The second one: `transformers` (formerly `pytorch-transformers`), came from PyTorch but adding tensorflow support also. \n",
        "\n",
        "For more classic work on Word-embeddings the gensim package which you worked with last week also has some decent ressorces. \n",
        "\n",
        "In this exercise set we will practice loading and applying models from both `tensorflow_hub` and from `transformers`. We will practice using sentence/paragraph embeddings as input to a clustering algorithm, as pretrained element in a new model, and finally try out the transformers library for pretraining a language model from scratch. \n",
        "\n",
        "Again we will use the Toxicity dataset. See download instructions in [week 7 exercises](https://github.com/ulfaslak/sds_tddl_2020/blob/master/exercises/exercises(7)_Categorydev_Class.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "floIsAX0JBkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset\n",
        "import pandas as pd\n",
        "path2tox_data = '/content/drive/My Drive/lm/toxic_train.csv'\n",
        "df = pd.read_csv(path2tox_data)\n",
        "# subsample data to allow faster prototyping\n",
        "# df = df.sample(5000) # simple solution\n",
        "# stratified solution where we subsample from each meta data column to get a higher variance.\n",
        "strat_sample_cols = df.columns[3:23]\n",
        "samples = []\n",
        "n = 300\n",
        "for col in strat_sample_cols:\n",
        "    binary = pd.DataFrame((df[col]>0.5).astype(int))\n",
        "    samples+=[j for _,j in binary.groupby(col).apply(lambda x: x.sample(min(len(x),n//2))).index]\n",
        "idx = list(set(samples))\n",
        "df = df.iloc[idx]\n",
        "\n",
        "# subsample for clustering\n",
        "df['label'] = (df.target>0.5).astype(int)\n",
        "\n",
        "sample = df.groupby('label').apply(lambda x: x.sample(500))\n",
        "sample_texts = sample.comment_text.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67gx4mRJ2fz",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 8.1:** *Pretrained Sentence Representations for Discovery and Exploration, using tensorflow_hub and the Universal Sentence Encoder*\n",
        "TFhub allows you plug and play with fully implemented pipelines, including preprocessing and the embedding forward pass. You will use this as a basic feature extractor similarly to what you did in Exercise 5 with image data.\n",
        "\n",
        "You will need to install tf_hub: `pip install --upgrade tensorflow-hub` first.\n",
        "\n",
        "> **Ex. 8.1.1:** Load and aply the [\"Universal Sentence Encoder\"](https://arxiv.org/abs/1803.11175) embedder using tensorflow hub.\n",
        "  - first `import tensorflow_hub as hub`\n",
        "  - define the \"embedder\" object using the `hub.Module()` function that takes a link to a pretrained module, and initializes it. Use the link: https://tfhub.dev/google/universal-sentence-encoder/4. (**Hint**: follow the link to see an example)\n",
        "  - Embed / transform a sample of texts from the toxicity dataaset to vectors by applying the embedder to a list of texts. Rremember to run the process within a tensorflow session:\n",
        "  ```\n",
        "  with tf.Session() as session:\n",
        "      session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTaiOYprLngf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjrAPGdgJfjb",
        "colab_type": "code",
        "outputId": "35223f85-150d-4d60-a611-a9461e0d2034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "### Load universal encoder."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTJSJ-6sL8ST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize tf and embed documents."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDY5-BVXM3Wu",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 8.1.2:** Exploration based on Document embeddings. \n",
        "  - First we use this for **exploring** similar texts. This time we can not just use `gensim`'s neat `.most_similar` function. Instead we contruct it ourselves.\n",
        "  - Construct a distance matrix between all texts. Here you can use the `sklearn.metrics.pairwise_distances` function that allows you to specify any distance measure implemented in the sklearn.metrics.pairwise. Get list here (`sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`) \n",
        "  - Now we have a matrix where each document has a row that expresses the distance to every other document. Now we brush up on our matrix manipulations skills. We want to transform the distance matrix into a matrix that express which document are closest to each other, i.e. each row will be sorted indices relating to the closests documents. Use the `.argsort` function built in to the matrix. Validate that the argsort() is correct.\n",
        "  - Now pick a random document contained in the distance matrix (i.e. a random index in the matrix). Print the text, along with with the most similar document (i.e. first index in the argsort matrix) and the distance score. Comment on what the model might have found / encoded.  \n",
        "  - Finally write a function that takes a document not contained in the distance matrix already defined, but instead embeds the document, calculates the distance to sample of texts already embedded, returns the top k closests documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEVi1m-WK9Bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create doc2doc matrix."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXiKYIk5WNah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply argsort."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj3D9d0aYdj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get closests neighbors of a random document."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcmuIg-SZcuL",
        "colab_type": "text"
      },
      "source": [
        "The similarity seems to be related to invoking religious symbols. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlCug7RyZ8mM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define function that has a document(i.e. userdefined string) as input and prints the 5 most similar documents in the dataset. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nchG8tKVddMh",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 8.1.3:** Discovery based on Document embeddings. Cluster and summarize.  \n",
        "  - Apply a clustering algorithm on the embeddings. `import sklearn.cluster`\n",
        "  - Now we want to inspect the clusters. \n",
        "    - Random sample: Do a random sample from the largest cluster.\n",
        "    - Most Representative: This will count as the Document with the shortest distance to all other docs. I.e. calculate average intra-cluster distance for each doc. Calculate the average intracluster distance for each document, and print the top 3 documents of the largest cluster.).\n",
        "\n",
        ">## Wordbased Summarizations\n",
        "Here we inspect the most representative words using TDIDF style weighing of each phrase/word in the cluster and in line with the \"Computer Assisted Keyword and Documentset Discovery\" we rank words in relation to feature importance / predictive capabilities. \n",
        "\n",
        ">**TfIdf style weighing**:. Idea is to calculate TDIDF not based on documents but on clusters. Formula is the following: \n",
        "$tfidf_{w,c} = tf_{w,c} \\cdot log(\\frac{\\left | N_{c} \\right |}{\\left | CC_w \\right |})$ where $N_c$ is no. of clusters $CC_w$ is no. of clusters word is present in. $tf_{w,c}$ is the frequency of a word in a cluster.\n",
        "  - Transform documents into a DocumentTermMatrix,i.e. Counts of Words and Phrases., using `sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,4),min_df=5). Add ngrams to allow longer phrases.\n",
        "  - Extrac the word index from the vectorizer using the function `.get_feature_names()`. This shall be used when printing the words, by translating an columnindex in the DocumentTermMatrix to a word.\n",
        "  - Transform the DocumentTermMatrix into a ClusterTermMatrix by summing accross each cluster.\n",
        "  - Transform this into a ClusterTermFrequencyMatrix by dividing by the sum for each cluster.\n",
        "  - Calculate the \"Inverse Cluster Frequency\". \n",
        "  - Multiply these together to form the TFIDF. \n",
        "  - For each cluster get sort each word by their tfidf score and  print the top 10 terms. Remember the word index you defined earlier.\n",
        "\n",
        ">## Extra \"Computer-Assisted Keyword and Document Set Discovery from Unstructured Text\" style word weighing. \n",
        "The method for discovering new query terms and in our context, phrases for doing *weak supervision', can also be used to summarize a given cluster.\n",
        "- Train a model (e.g. `sklearn.linear_model.LogisticRegression`) using the DocumentTermMatrix as input and the cluster labels as output.\n",
        "- Extract the coefficients of the model using the `.coef_` property of the model object.\n",
        "- For each cluster label sort the words by the largest coefficients.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utLHmib6dzeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cluster using sklearn.cluster"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXozoIGVfeWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect random sample of documents from largest cluster"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaKnRyinqzco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find most representative documents by calculating\n",
        "# the average intra-cluster-distance for each \n",
        "# document to its cluster neighbors. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9b4fpU1fuga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Word Summarization\n",
        "# Summarize by printing TF-IDF weighed words."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4SajBgmhiYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print top \"phrases\" of each cluster based on the tfidf score."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UguVMo0VhTVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  \"Computer Assisted document and set discovery\" most predictive summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5BEGQSZ1E8e",
        "colab_type": "text"
      },
      "source": [
        "## Transfer Learning for supervised learning\n",
        "**Ex. 8.2:** Adopt pretrained embeddings into a larger model.\n",
        "Here we shall practice using pretrained models as part of a larger pipeline using tensorflow_hub and keras. \n",
        ">\n",
        "**Ex. 8.2.1:** Built a Keras model where the first layer is the Universal Sentence Encoder, and stack layers on top.\n",
        "  - initialize your model as using the `Sequential()` function.\n",
        "  - add the hub layer using the `hub.KerasLayer(module_url,input_shape=[],dtype=tf.string,trainable=False)` \n",
        "    - `trainable = True` option allows you to finetune the Universal Sentence Encoder also, this however will slow down training significantly. \n",
        "  - Add a classification layer on top. You may add any layers you like.\n",
        "  - Compile model.\n",
        "  - Train model. Because it is using Tensorflow again you need to initialize the session, using the line: ```with tf.Session() as sess:\n",
        "  sess.run([tf.global_variables_initializer(), tf.tables_initializer()])```. And then call the `.fit()` method inputting your training data.\n",
        "` \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyvWwODi2IGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adopt Universal Sentence Encoder \n",
        "# as the first layer in preprocessing \n",
        "# step in a larger Keras Pipeline\n",
        "\n",
        "# Prepare Dataset split the Toxicity dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['comment_text'], \n",
        "                                                    df['label'], \n",
        "                                                    test_size=0.3, \n",
        "                                                    stratify=df['label'], \n",
        "                                                    random_state=42)\n",
        "import numpy as np\n",
        "val_dat = (np.array([i for i in x_test.values]), y_test.values)\n",
        "\n",
        "# Initialize Keras Sequetial Pipeline\n",
        "\n",
        "# Add USE layer to model\n",
        "# Define extra layers for the pipeline.\n",
        "# Compile model.\n",
        "              \n",
        "# Run session and fit model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7z_iH4bVj5R",
        "colab_type": "text"
      },
      "source": [
        "See also this module (https://pypi.org/project/keras-lr-multiplier/) for doing discriminate fintuning (i.e. different learning rates for each layer), as described in the [\"Universal Language Model Fine-tuning for Text Classification\"](https://arxiv.org/pdf/1801.06146.pdf)(Howard and Ruder 2017)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras_lr_multiplier import LRMultiplier\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(\n",
        "    units=5,\n",
        "    input_shape=(5,),\n",
        "    activation='tanh',\n",
        "    name='Dense',\n",
        "))\n",
        "model.add(Dense(\n",
        "    units=2,\n",
        "    activation='softmax',\n",
        "    name='Output',\n",
        "))\n",
        "model.compile(\n",
        "    optimizer=LRMultiplier('adam', {'Dense': 0.5, 'Output': 1.5}),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUIyOJipHHpf",
        "colab_type": "text"
      },
      "source": [
        "## Testdriving the Transformers Library\n",
        "> Exercise 8.3.2: Train a Language Model from scratch.\n",
        "- First we need to compile a dataset. Famously wikipedia data has been used, becasue it is available in many languages. Wikimedia provides data dumps regularly. Choose a language to download from the https://dumps.wikimedia.org, e.g. the danish wikpedia: https://dumps.wikimedia.org/dawiki/20200101/dawiki-20200101-pages-articles.xml.bz2\n",
        "- Next we should unzip it, and preprocess it to extract plain text. See code below.\n",
        "- To further prepare it for training the language model we should split the text into train and eval. We will do this by running through all files, and writing to a train file with a probability of p, and a eval file with a probability of 1-p. \n",
        "  - For each article (i.e. line in file), get a random value using the `random.random()` function. If below p write to test, if above write to train file.\n",
        "\n",
        "Now you should follow the Tutorial Provided by the huggingface organization and the Implementation found here: https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
        "or appropriate the following implementation https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\n",
        "\n",
        "\n",
        "- **Important Update** as this is an open source world, the implementations are changing and the example is not fully up-to-date with the newest changes to the `transformers` package api. With the transformers 2.6 release certain changes that have not been implemented in their `run_language_modelling.py` training script. To work around this issue I have put in a cell implementing a somewhat *crazy* hack, to slightly alter the script.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY4J4E4B-75E",
        "colab_type": "code",
        "outputId": "08d82758-d1a6-4570-deca-fc3afc47ab0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "# Download language modelling corpus from the wikimedia.org \n",
        "data_link = 'https://dumps.wikimedia.org/dawiki/20200101/dawiki-20200101-pages-articles.xml.bz2' # define link\n",
        "\n",
        "# download data\n",
        "! wget {data_link}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-26 14:16:00--  https://dumps.wikimedia.org/dawiki/20200101/dawiki-20200101-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 325201125 (310M) [application/octet-stream]\n",
            "Saving to: ‘dawiki-20200101-pages-articles.xml.bz2’\n",
            "\n",
            "dawiki-20200101-pag 100%[===================>] 310.14M  4.62MB/s    in 65s     \n",
            "\n",
            "2020-03-26 14:17:05 (4.77 MB/s) - ‘dawiki-20200101-pages-articles.xml.bz2’ saved [325201125/325201125]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY8RfBs1KiCY",
        "colab_type": "code",
        "outputId": "71a536a7-e1e8-45ec-b194-53ffbf5c9a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# unzip data\n",
        "filename = data_link.split('/')[-1]\n",
        "! bzip2 -d {filename}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bzip2: Output file dawiki-20200101-pages-articles.xml already exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJg3QJh_LFlu",
        "colab_type": "code",
        "outputId": "22f917d2-54f6-4a44-ccd1-e6bc43142ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Inspect data\n",
        "unzipped_file = filename.split('.bz2')[0]\n",
        "# inspect file\n",
        "! head -400 {unzipped_file}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"da\">\n",
            "  <siteinfo>\n",
            "    <sitename>Wikipedia</sitename>\n",
            "    <dbname>dawiki</dbname>\n",
            "    <base>https://da.wikipedia.org/wiki/Forside</base>\n",
            "    <generator>MediaWiki 1.35.0-wmf.11</generator>\n",
            "    <case>first-letter</case>\n",
            "    <namespaces>\n",
            "      <namespace key=\"-2\" case=\"first-letter\">Media</namespace>\n",
            "      <namespace key=\"-1\" case=\"first-letter\">Speciel</namespace>\n",
            "      <namespace key=\"0\" case=\"first-letter\" />\n",
            "      <namespace key=\"1\" case=\"first-letter\">Diskussion</namespace>\n",
            "      <namespace key=\"2\" case=\"first-letter\">Bruger</namespace>\n",
            "      <namespace key=\"3\" case=\"first-letter\">Brugerdiskussion</namespace>\n",
            "      <namespace key=\"4\" case=\"first-letter\">Wikipedia</namespace>\n",
            "      <namespace key=\"5\" case=\"first-letter\">Wikipedia-diskussion</namespace>\n",
            "      <namespace key=\"6\" case=\"first-letter\">Fil</namespace>\n",
            "      <namespace key=\"7\" case=\"first-letter\">Fildiskussion</namespace>\n",
            "      <namespace key=\"8\" case=\"first-letter\">MediaWiki</namespace>\n",
            "      <namespace key=\"9\" case=\"first-letter\">MediaWiki-diskussion</namespace>\n",
            "      <namespace key=\"10\" case=\"first-letter\">Skabelon</namespace>\n",
            "      <namespace key=\"11\" case=\"first-letter\">Skabelondiskussion</namespace>\n",
            "      <namespace key=\"12\" case=\"first-letter\">Hjælp</namespace>\n",
            "      <namespace key=\"13\" case=\"first-letter\">Hjælp-diskussion</namespace>\n",
            "      <namespace key=\"14\" case=\"first-letter\">Kategori</namespace>\n",
            "      <namespace key=\"15\" case=\"first-letter\">Kategoridiskussion</namespace>\n",
            "      <namespace key=\"100\" case=\"first-letter\">Portal</namespace>\n",
            "      <namespace key=\"101\" case=\"first-letter\">Portaldiskussion</namespace>\n",
            "      <namespace key=\"102\" case=\"first-letter\">Artikeldata</namespace>\n",
            "      <namespace key=\"103\" case=\"first-letter\">Artikeldatadiskussion</namespace>\n",
            "      <namespace key=\"828\" case=\"first-letter\">Modul</namespace>\n",
            "      <namespace key=\"829\" case=\"first-letter\">Moduldiskussion</namespace>\n",
            "      <namespace key=\"2300\" case=\"first-letter\">Gadget</namespace>\n",
            "      <namespace key=\"2301\" case=\"first-letter\">Gadget talk</namespace>\n",
            "      <namespace key=\"2302\" case=\"case-sensitive\">Gadget definition</namespace>\n",
            "      <namespace key=\"2303\" case=\"case-sensitive\">Gadget definition talk</namespace>\n",
            "    </namespaces>\n",
            "  </siteinfo>\n",
            "  <page>\n",
            "    <title>Amter</title>\n",
            "    <ns>0</ns>\n",
            "    <id>1</id>\n",
            "    <redirect title=\"Amt\" />\n",
            "    <revision>\n",
            "      <id>7488148</id>\n",
            "      <parentid>2022085</parentid>\n",
            "      <timestamp>2014-02-03T21:43:11Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Palnatoke</username>\n",
            "        <id>1053</id>\n",
            "      </contributor>\n",
            "      <comment>#REDIRECT [[Amt]]</comment>\n",
            "      <model>wikitext</model>\n",
            "      <format>text/x-wiki</format>\n",
            "      <text xml:space=\"preserve\">#REDIRECT [[Amt]]</text>\n",
            "      <sha1>eco50ip08cfjn6rloq8a7g76sz4xt48</sha1>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>Arkæologi</title>\n",
            "    <ns>0</ns>\n",
            "    <id>2</id>\n",
            "    <revision>\n",
            "      <id>9672017</id>\n",
            "      <parentid>9671303</parentid>\n",
            "      <timestamp>2018-09-15T13:21:39Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Toxophilus</username>\n",
            "        <id>67353</id>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <comment>Småret</comment>\n",
            "      <model>wikitext</model>\n",
            "      <format>text/x-wiki</format>\n",
            "      <text xml:space=\"preserve\">[[Fil:Archäologie schichtengrabung.jpg|thumb|Arkæologisk [[udgravning]] med profil.]]\n",
            "\n",
            "'''Arkæologi''' er studiet af tidligere tiders [[menneske]]lige [[aktivitet]], primært gennem studiet af menneskets materielle levn. Langt det meste af al menneskelig aktivitet foregik, før vi lærte at skrive, så arkæologi er den vigtigste metode til at studere ældre menneskeskabte samfund. \n",
            "\n",
            "== Metoder ==\n",
            "[[File:Arkeologi - Ystad-2018.jpg|thumb||Arkæologisk [[udgravning]] ved [[Ystad Kloster]] i 2018.]]\n",
            "\n",
            "Udgravninger kan foretages på flere måder. Den traditionelle (engelske) arkæologi gik ud fra princippet &quot;find en væg og følg den&quot; (&quot;''find a wall and follow it''&quot;). Nyere arkæologi består som hovedregel af flere led:\n",
            "#overfladeafsøgninger, hvor arkæologen undersøger et større område for arkæologiske fund og markerer dem,\n",
            "#profilgravninger, hvor arkæologen på bestemte steder lader et lodret profil stå for derved at kunne danne sig et indtryk af de lagserier ([[stratigrafi]]), som findes det pågældende sted,\n",
            "#fladeafgravninger, hvor arkæologen udgraver et større eller mindre område og lader alle arkæologisk fund registrere og deres beliggenhed indmåle tredimensionelt for efterfølgende at kunne rekonstruere sammenhængende fund og den eventuelle spredning af dem, der måtte være fundet sted.\n",
            "\n",
            "Som et led i det arkæologiske arbejde indgår også beskrivelsen af fundene, rekonstruktionen af dem på grundlag af fundne spor og brudstykker (fx af itubrudte lerkar) samt bestemmelse af hvilke(n) type(r), de(t) tilhører og dermed også en tidsbestemmelse og henføring til en [[Kultur# Kultur i forskellige former|arkæologisk kultur]].\n",
            "\n",
            "Arkæologi inddrager mange forskellige hjælpediscipliner i beskrivelsen af vores historie, både de materielle levn som er efterladt, og de skriftlige og billedlige overleveringer eksempelvis i form af [[arkæologisk illustration|arkæologiske illustrationer]], således rekonstruktioner. Naturvidenskabelige undersøgelsesmetoder er i stadig udvikling og inddrages i stort omfang. Til datering benyttes eksempelvis [[dendrokronologi]], [[kulstof 14-datering]], [[kornaftryk]] og [[pollenanalyse]]. I visse tilfælde benyttes også [[filologi]] til at skabe sammenhænge mellem historiske og arkæologiske kilder.\n",
            "\n",
            "== Arkæologiske discipliner ==\n",
            "\n",
            "Arkæologi beskæftiger sig med den lange periode – som i Danmark begynder med menneskets indvandring til landet for ca. 10-15.000 år siden til vikingetidens ophør omkring slutningen af 1000 tallet e. Kr. og endda senere. Arkæologien kan derfor underinddeles i en række typer alt efter hvilken tid, hvilket emne og hvilket område, der undersøges:\n",
            "*Forhistorisk arkæologi arbejder med forhistoriske perioder ([[stenalder]], [[bronzealder]], [[jernalder]]).\n",
            "*Middelalderarkæologi arbejder med den sene vikingetid fra 900/1000 tallet til reformationen i [[1536]], og nogle middelalderarkæloger medtager også [[renæssancen]] til midten af 1600 tallet. Det er almindeligt, at arkæologer også interesserer sig for 17- og 1800 tallet. \n",
            "*[[Klassisk arkæologi]] dækker [[antikken]] i [[oldtidens Grækenland]] og [[Romerriget]]. Derudover findes [[nærorientalsk arkæologi]], der omfatter områderne i [[Mellemøsten]] og de stor [[højkultur]]er i [[Mesopotamien]]. Både klassisk og nærorientalsk arkæologi arbejder også med materielle levn, men er påvirkede af både [[kunsthistorie]] og [[kulturhistorie]]. \n",
            "*[[Ægyptologi]], der udelukkende beskæftiger sig med [[Det gamle Egypten]]. \n",
            "*Ved hjælp af [[eksperimentel arkæologi]] kan man rekonstruere fundne genstand, og afprøve dem i praksis.\n",
            "*[[Slagmarksarkæologi]] undersøger områder, hvor man ved at der har foregået [[Slag (krig)|slag]]. Dette kan strække sig over store perioder fra [[Varusslaget]] i år [[9]], til [[Slaget ved Agincourt]] i 1415 og frem til slag under [[2. verdenskrig]].\n",
            "*[[Marinarkæologi]] er den gren af arkæologien der beskæftiger sig med menneskets relation til havet. Undervandsarkæologi omhandler særskilt den del af marinarkæologien, der påtræffes submarint.\n",
            "*[[Bibelsk arkæologi]] forsøger at finde arkæologiske genstande, der understøtter biblen.\n",
            "*[[Etnoarkæologi]] betegner forskning, hvor man ved besøg hos folkeslag, der endnu lever på en lignende måde som de folk, der arkæologisk udgraves, formodes at have levet, forsøger at danne sig et bedre indtryk af samfundsopbygning og tankegang hos forhistoriske folk. Fremgangsmåden er omstridt blandt arkæologer.\n",
            "Det siger sig selv, at vilkårene ved arkæologiske udgravninger vil variere meget fra sted til sted og fra udgravning til udgravning.\n",
            "\n",
            "== Arkæologisk kulturforståelse ==\n",
            "\n",
            "Arkæologien har lånt det [[etnografi]]ske kulturbegreb og anvender dettes grupper til at beskrive et givet forhistorisk samfunds overordnede indretning. Som identificeringsbetegnelse anvendes kultur imidlertid i arkæologisk sammenhæng for at betegne en i tid og udbredelse given levevis, fx [[Hellenisme|Hellenistisk kultur]], [[Kurgan-hypotesen|Kurgankulturen]] eller [[Ertebøllekulturen]]. \n",
            "\n",
            "Den arkæologiske kulturbetegnelse har sin rod i oprindelsen af den moderne arkæologi fra omkring midten af 1800-tallet. Da man gik over til at danne sig billedet af fortidens levevis på grundlag af de jordfund af genstande, beboelseslevn, grave og knogler, som man begyndte at gøre, var det naturligt at knytte en betegnelse til sådanne iagttagne fund for derved at kunne henvise til dem når nye, lignende fund blev gjort. Den første arkæologiske skelnen blev indført af [[Christian Jürgensen Thomsen]], der opdelte genstande efter deres materialegrundlag i henholdsvis stenalder, bronzealder og jernalder.&lt;ref&gt;Bibby (1980), s. 15, 70&lt;/ref&gt; Snart viste det sig, at denne opdeling ikke var tilstrækkelig for at udskille ulige udformninger af samme slags genstande, og behovet for yderligere inddelinger blev snart klart. En af de tidligste underinddelinger var i &quot;den uslebne stenalder&quot; (nu almindeligvis benævnt [[jægerstenalder]]) og &quot;den slebne stenalder&quot; (nu almindeligvis benævnt [[bondestenalder]]).&lt;ref&gt;Bibby (1980), s. 80&lt;/ref&gt; I begyndelsen var antallet af sådanne betegnelser mangfoldige, og flere betegnelser kunne anvendes side om side om samme kultur. Eksempelvis anvendes betegnelserne stridsøksekultur og enkeltgravskultur endnu i dag om samme kulturform. Nogle gange skelnedes mellem et ældre og et yngre kulturtrin (således endnu i dag skelnes mellem [[bronzealder|ældre bronzealder]] og [[bronzealder|yngre bronzealder]]). Det var den schweiziske forhistoriker [[Gabriel de Mortillet]], der gjorde sig til talsmand for at inddele karakteristiske arkæologiske fundkomplekser i kulturer som hovedregel - men ikke altid - navngivne efter det sted, hvor de først er fundet eller er mest udprægede.&lt;ref&gt;Bibby (1980), s. 42&lt;/ref&gt;\n",
            "\n",
            "Kendetegnende for arkæologiske kulturer er forekomsten af materielle ''ledetyper'' det vil sige særegne og let genkendelige udformninger af redskaber, begravelsesformer eller lignende. Tidligere anvendte arkæologiske benævnelser som &quot;tragtbægerkultur&quot;, &quot;dolktid&quot; og &quot;stridsøksekultur&quot; henviste således til henholdsvis lerkar og våben med karakteristiske former, mens benævnelser som &quot;enkeltgravskultur&quot; og &quot;hellekistetid&quot; henviste til særegne gravformer.&lt;ref&gt;Tauber, s. 28&lt;/ref&gt;\n",
            "\n",
            "Til trods for, at denne kulturinddeling udvikledes allerede i løbet af det 19. århundrede, var det først i begyndelsen af det 20. århundrede, at arkæologerne for alvor begyndte at formulere principperne bag den anvendte fremgangsmåde. Det er i første række den australske arkæolog [[Vere Gordon Childe]], der konstaterede, at visse materielle fund af våben, smykker, fartøjer, begravelsesritualer og bygningsformer atter og atter blev fundet i de samme sammensætninger og dermed kunne berettige til at bruge betegnelsen kultur for derved at betegne den levevis og den befolkning, der havde frembragt dem.\n",
            "\n",
            "Dette fik den tyske arkæolog [[Gustaf Kossinna]] (1858-1931) til at mene, at han kunne opstille en entydig forbindelse mellem en arkæologisk kulturprovins, en etnisk gruppe og en sproglig enhed.&lt;ref&gt;Lund, s. 114&lt;/ref&gt; Skønt dette i mange tilfælde kan have en vis rigtighed, er det vigtigt altid at være opmærksom på de forbindelser, der kan være med andre kultursamfund. De kan foranledige, at redskaber og redskabstyper, byggeskik eller begravelsesmåde spredes fx som handelsvarer, gaver, medgift, krigsbytte eller besøgsindtryk. Man må derfor altid holde sig for øje, om det kun er enkelte genstande eller træk i levevis, der er ens flere steder, eller om det er den samlede levemåde. \n",
            "\n",
            "Et andet område, hvor arkæologerne med tiden har ændret holdning, er, når de skal forklare årsagerne til, at nye kulturer afløser gamle. Tidligere mente man, at sådanne kulturskift måtte forklares ved indvandringer, hvor den tidligere kultur eller livsform blev fortrængt eller undertrykket. I nyere tid ser man dog oftere sådanne kulturskifter forklaret ved, at nye ideer eller teknikker har spredt sig fra et samfund til et andet og formoder, at egentlige folkevandringer har været få, små og ubetydelige. Endnu i dag er der mange kulturskifter, hvor arkæologerne ikke kan blive enige om betydningen af folkevandringer. Blandt de mest omstridte emner er spørgsmålet om stridsøksefolkets eller [[enkeltgravskultur]]ens mulige indvandring (eller ej) til Vesteuropa fra sletterne nord for [[Sortehavet]].\n",
            "\n",
            "Et andet nyt træk i arkæologien er, at man i mindre grad bruger udtrykket &quot;kultur&quot; og i stedet taler om &quot;teknokomplekser&quot; (engelsk: ''Techno-Complex'', en forkortelse for Technology-Complexes) for derved at understrege, at man alene betegner genstandskulturen men ikke nødvendigvis den livsform eller den befolkning (herunder i sproglig eller etnisk/national henseende), der har skabt den.\n",
            "\n",
            "== Noter ==\n",
            "{{Reflist}}\n",
            "== Litteratur ==\n",
            "* [[Geoffrey Bibby]]: ''I Dilmun tier ravnen''; Wormanium 1971; ISBN 87-85-1600-40\n",
            "* Geoffrey Bibby: ''Spadens vidnesbyrd''; Wormanium 1980; ISBN 87-8516-071-7\n",
            "* Allan A. Lund: De etnografiske kilder til Nordens tidlige historie; Wormanium 1993; ISBN 87-89531-08-6\n",
            "* Henrik Tauber: &quot;Det store hvornår? Om den svære kunst at datere fortiden&quot; ([[Kronik]] i ''[[Skalk]]'' 1972 Nr. 1; s. 22-29)\n",
            "\n",
            "== Se også ==\n",
            "[[Fil:Washingskul.png|thumb|En arkæologisk prøve.]]\n",
            "* Primært om [[skrift]]er og [[skriftsprog]]:\n",
            "** [[sanskrit]]\n",
            "*** [[Vediske skrifter]]\n",
            "** [[hieroglyf]]fer\n",
            "*** [[Rongorongo]]\n",
            "** [[Runealfabet|runer]]\n",
            "*** [[Runesten]]\n",
            "** [[Rosettestenen]]\n",
            "** [[Helleristning]]\n",
            "** [[Linear A]]\n",
            "** [[Linear B]]\n",
            "** [[Bibelen]]\n",
            "*** [[Dødehavsrullerne]]\n",
            "*** De [[apokryfe skrifter]]\n",
            "** [[Koranen]]\n",
            "* Inskriptioner med samme indhold på flere sprog:\n",
            "** [[Bisutun-inskriptionerne]]\n",
            "** [[Fuente Magna og Pokotia-monolitten]]\n",
            "** [[Rosettestenen]]\n",
            "* Primært om anden [[vidnesbyrd]]:\n",
            "** [[stendysse]]\n",
            "** [[dysse]]\n",
            "** [[Bautasten]]\n",
            "** [[Trelleborge]]\n",
            "** [[Ertebøllekulturen]]\n",
            "** [[Køkkenmødding]]er\n",
            "** [[Fossil]]\n",
            "** [[Forstening]]\n",
            "** [[Stonehenge]]\n",
            "** [[Nazca-linjerne]]\n",
            "** [[Sfinksen i Giza]]\n",
            "** [[Pyramiderne i Giza]]\n",
            "** [[Den store pyramide i Giza]]\n",
            "** [[Verdens syv underværker]]\n",
            "** [[Lascaux-hulerne]]\n",
            "** [[Minoisk civilisation]]\n",
            "\n",
            "== Eksterne henvisninger ==\n",
            "{{Commonscat|Archaeology}}\n",
            "\n",
            "{{Navboks Arkæologi}}\n",
            "{{autoritetsdata}}\n",
            "\n",
            "[[Kategori:Arkæologi| ]]\n",
            "[[Kategori:Akademiske discipliner]]\n",
            "[[Kategori:Videregående uddannelser i Danmark]]</text>\n",
            "      <sha1>1urjkrwtjklmpyqlrk6olp6iutxafdf</sha1>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>Asien</title>\n",
            "    <ns>0</ns>\n",
            "    <id>3</id>\n",
            "    <revision>\n",
            "      <id>10142997</id>\n",
            "      <parentid>9976221</parentid>\n",
            "      <timestamp>2019-11-25T17:37:23Z</timestamp>\n",
            "      <contributor>\n",
            "        <ip>90.184.212.62</ip>\n",
            "      </contributor>\n",
            "      <comment>/* Lande i Asien */</comment>\n",
            "      <model>wikitext</model>\n",
            "      <format>text/x-wiki</format>\n",
            "      <text xml:space=\"preserve\">[[Fil:LocationAsia.png|thumb|Verdenskort med Asien fremhævet.]]\n",
            "'''Asien''' er verdens største [[kontinent]], med et [[areal]] på cirka 44,58 millioner km².\n",
            "\n",
            "Kontinentet grænser mod nord, øst og syd til [[verdenshav]]et: [[Ishavet|Det Arktiske hav]], [[Stillehavet]] og [[Det Indiske Ocean]].\n",
            "Afgrænsningen mellem Europa og Asien er ikke fast defineret, men angives typisk at gå gennem [[Uralbjergene]], [[Uralfloden]], det [[Kaspiske Hav]], [[Kaukasus]], [[Sortehavet]], [[Bosporus]], [[Marmarahavet]] og [[Dardanellerne]] (øst for [[Det Ægæiske Hav]]) og [[Det Røde Hav]].\n",
            "Man kan også sige det er [[superkontinent]]et [[Afrika-Eurasien]] uden [[Europa]] og [[Afrika]].\n",
            "\n",
            "Betegnelsen 'Asien' har været brugt siden [[Forhistorisk tid#Studiet af forhistorisk tid|oldtiden]]. Oprindelsen tilskrives det [[Hittitterne|hittitisk]]e ''assu'' (=solopgang, øst).&lt;ref&gt;[https://www.papagei.com/de/magazin/welt-der-sprachen/laendernamen/ Wie Länder zu ihrem Namen gekommen sind], hentet 12. maj 2018&lt;/ref&gt;\n",
            "\n",
            "== Lande i Asien ==\n",
            "{|width=80%\n",
            "|-valign=top\n",
            "|width=33%|\n",
            "* Der er 50 lande i Asien\n",
            "*[[Afghanistan]]\n",
            "* [[Armenien]]\n",
            "* [[Aserbajdsjan]]\n",
            "* [[Bahrain]]\n",
            "* [[Bangladesh]]\n",
            "* [[Bhutan]]\n",
            "* [[Brunei]]\n",
            "* [[Cambodja]]\n",
            "* [[Filippinerne]]\n",
            "* [[Forenede Arabiske Emirater]]\n",
            "* [[Georgien]]\n",
            "* [[Indien]]\n",
            "* [[Indonesien]]\n",
            "* [[Irak]]\n",
            "* [[Iran]]\n",
            "* [[Israel]]\n",
            "* [[Japan]]\n",
            "|width=33%|\n",
            "* [[Jordan]]\n",
            "* [[Kasakhstan]] (også i Europa)\n",
            "* [[Kina]]\n",
            "* [[Kirgisistan]]\n",
            "* [[Kuwait]]\n",
            "* [[Laos]]\n",
            "* [[Libanon]]\n",
            "* [[Malaysia]]\n",
            "* [[Maldiverne]]\n",
            "* [[Mongoliet]]\n",
            "* [[Myanmar]] (tidligere Burma)\n",
            "* [[Nepal]]\n",
            "* [[Nordkorea]]\n",
            "* [[Oman]]\n",
            "* [[Pakistan]]\n",
            "* [[Palæstina]] (områdets status omtvistet)\n",
            "|width=37%|\n",
            "* [[Qatar]]\n",
            "* [[Rusland]] (også i Europa)\n",
            "* [[Saudi-Arabien]]\n",
            "* [[Singapore]]\n",
            "* [[Sri Lanka]]\n",
            "* [[Sydkorea]]\n",
            "* [[Syrien]]\n",
            "* [[Tadsjikistan]]\n",
            "* [[Taiwan]]\n",
            "* [[Thailand]]\n",
            "* [[Tibet]] (ifølge FN en del af Kina)\n",
            "* [[Turkmenistan]]\n",
            "* [[Tyrkiet]] (også i Europa)\n",
            "* [[Usbekistan]]\n",
            "* [[Vietnam]]\n",
            "* [[Yemen]]\n",
            "* [[Østtimor]]\n",
            "|}\n",
            "\n",
            "== Regioner i Asien ==\n",
            "* [[Mellemøsten]] ([[Nærorienten]])\n",
            "** [[Lilleasien]] ([[Anatolien]])\n",
            "** [[Cypern]]\n",
            "** [[Levanten]]\n",
            "** Den [[Arabiske Halvø]]\n",
            "** [[Kaukasus]]\n",
            "** Den [[Iranske Højslette]]\n",
            "* [[Sydasien]] (det Indiske Subkontinent)\n",
            "* [[Sydøstasien]]\n",
            "** [[Indokina]]\n",
            "** Den [[Malayahalvøen|Malayiske Halvø]]\n",
            "* [[Centralasien]]\n",
            "* [[Nordasien]]\n",
            "* [[Østasien]] ([[Fjernøsten]])\n",
            "\n",
            "== Referencer ==\n",
            "{{reflist}}\n",
            "\n",
            "== Se også ==\n",
            "{{søsterlinks\n",
            "|commonskat = Asia\n",
            "|wikt = Asien\n",
            "|voyage = Asien\n",
            "}}\n",
            "* [[Asiatiske flag]]\n",
            "* [[Verdens lande]]\n",
            "\n",
            "{{Reflist}}\n",
            "\n",
            "{{Verdens regioner}}\n",
            "{{autoritetsdata}}\n",
            "{{coord wd|type=country}}\n",
            "\n",
            "[[Kategori:Asien| ]]</text>\n",
            "      <sha1>i5glsk4ku0j9ry0tvh523avu3ozu3mi</sha1>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>Aalborg Universitet Esbjerg</title>\n",
            "    <ns>0</ns>\n",
            "    <id>4</id>\n",
            "    <revision>\n",
            "      <id>9416311</id>\n",
            "      <parentid>8927737</parentid>\n",
            "      <timestamp>2018-03-02T13:33:35Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Toxophilus</username>\n",
            "        <id>67353</id>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <comment>Fjernede [[Kategori:Bygninger, konstruktioner og anlæg i Esbjerg]] ved hjælp af [[Hjælp:HotCat|Hotcat]]</comment>\n",
            "      <model>wikitext</model>\n",
            "      <format>text/x-wiki</format>\n",
            "      <text xml:space=\"preserve\">{{redirflertydig3|AUE|den tyske by|Aue (Sachsen)}}\n",
            "'''Aalborg Universitet Esbjerg''' (tidligere kendt som AUE og AAUE, men benævnes nu AAU-Esbjerg) er organisatorisk en afdeling (institut) under [[Aalborg Universitet]], men er geografisk lokaliseret i [[Esbjerg]]. Instituttets officielle navn er &quot;Esbjerg Tekniske Institut&quot;, tidligere &quot;Institut for Kemi og Anvendt Ingeniørvidenskab&quot;.\n",
            "\n",
            "AAU-Esbjerg danner p.t. ([[november]] [[2014]]) rammen om hverdagen for ca. 700 studerende og 110 ansatte. Universitetet uddanner primært [[diplomingeniør|diplom]], [[civilingeniør]]er, men også medialoger, selvom denne uddannelse blev permanent lukket for optag i 2015 grundet fremdriftsreformen. Tidligere har uddannelsen [[fiskeriteknologi]] haft base på AAU-Esbjerg, men denne er nu flyttet til Holstebro.\n",
            "\n",
            "Det nuværende institut blev skabt ved en fusion i [[1995]], mellem det daværende Ingeniørhøjskolen Esbjerg og Aalborg Universitet. Siden fusionen er antallet af elever steget støt, og der har været behov for udvidelser af de fysiske rammer i flere etaper. Senest i 2015 blev den nybyggede C2-fløj, hvori der er topmoderne laboratorier til både bl.a. Kemi- og Energi- uddannelserne, samt kontorer og grupperumsfaciliteter, indviet samtidig med en renovering og modernisering af B- og C1-fløjene. Derudover er der planer om omfattende renovering D-fløjen, som tidligere husede laborarorier. Disse planer er dog, af økonomiske årsager sat på pause på ubestemt tid.\n",
            "\n",
            "Især har en massiv satsningen på [[informationsteknologi]], med oprettelsen af en afdeling for software- og medieteknologi (Department of Software and Media Technology), samt uddannelser der er skræddersyet til olie- og [[offshore]]-branchen i [[Nordsøen]] tiltrukket mange studerende.\n",
            "\n",
            "Studenterorganisationen på AAU-Esbjerg hedder [[DSR-SE]], og studenterbaren &quot;Smuthullet&quot;.\n",
            "\n",
            "== Historisk ==\n",
            "Det der i dag er Aalborg Universitet Esbjerg, hviler på et fundament som blev påbegyndt i 1964 da Esbjerg Bygningsteknikum blev oprettet. Dette skete efter en årelang indsats med at overbevise [[centraladministration]]en om behovet for tekniske uddannelser i Esbjerg. En række lokale entreprenører havde et stort behov for teknisk kyndige medarbejdere til brug i deres virksomheder, og det lykkedes dem at overbevise København om at dette behov skulle opfyldes med en lokal ingeniøruddannelse.\n",
            "Dette forhindrede dog ikke en omtumlet tilværelse de første år, hvor institutionen flere gange var lukningstruet, men den fik lov at bestå, og kunne i [[1968]] flyttede ind i nyetablerede bygninger på stedet hvor universitetet også i dag er lokaliseret.\n",
            "\n",
            "== Ekstern henvisning ==\n",
            "* [http://www.esbjerg.aau.dk/ Aalborg Universitet Esbjerg]\n",
            "* [http://www.smuthullet.net/ Smuthullet]\n",
            "\n",
            "{{coord|55|29|29.65|N|8|26|50.38|E|type:landmark_region:DK|display=title}}\n",
            "{{DEFAULTSORT:Ålborg Universitet Esbjerg}}\n",
            "\n",
            "{{billedesavnes}}\n",
            "{{autoritetsdata}}\n",
            "\n",
            "[[Kategori:Aalborg Universitet]]\n",
            "[[Kategori:Uddannelsesinstitutioner i Esbjerg]]</text>\n",
            "      <sha1>nucgyv9ohlpmgbjvsrda1hdqiwcaipk</sha1>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>Anders Fogh Rasmussen</title>\n",
            "    <ns>0</ns>\n",
            "    <id>5</id>\n",
            "    <revision>\n",
            "      <id>10108845</id>\n",
            "      <parentid>10068389</parentid>\n",
            "      <timestamp>2019-10-30T11:58:27Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Toxophilus</username>\n",
            "        <id>67353</id>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <comment>Tilføjede [[Kategori:Studenter fra Viborg Katedralskole]] ved hjælp af [[Hjælp:HotCat|Hotcat]]</comment>\n",
            "      <model>wikitext</model>\n",
            "      <format>text/x-wiki</format>\n",
            "      <text xml:space=\"preserve\">{{Infoboks leder |wikidata=alle |ingen_wikidata= \n",
            "| navn               = Anders Fogh Rasmussen\n",
            "| billede            = Former Danish Prime Minister Anders Fogh Rasmussen at the Nordic Council Session in Helsinki 2008-10-28.jpg\n",
            "| billedtekst        = Anders Fogh Rasmussen i 2008.\n",
            "\n",
            "| embede             = [[NATO]]'s 12. [[NATO's generalsekretær|generalsekretær]]\n",
            "| embede_start       = [[1. august]] [[2009]]\n",
            "| embede_slut        = [[30. september]] [[2014]]\n",
            "| forgænger          = [[Jaap de Hoop Scheffer]]\n",
            "| efterfølger        = [[Jens Stoltenberg]]\n",
            "\n",
            "| embede2            = [[Danmark]]s [[Danske statsministre|39.]] [[Danmarks statsminister|statsminister]]\n",
            "| monark2            = [[Margrethe 2.]]\n",
            "| vicestatsminister2 = [[Bendt Bendtsen]] &lt;small&gt;(2001-2008)&lt;/small&gt;&lt;br /&gt;[[Lene Espersen]] &lt;small&gt;(2008-2009)&lt;/small&gt;\n",
            "| embede_start2      = [[27. november]] [[2001]]\n",
            "| embede_slut2       = [[5. april]] [[2009]]\n",
            "| forgænger2         = [[Poul Nyrup Rasmussen]]\n",
            "| efterfølger2       = [[Lars Løkke Rasmussen]]\n",
            "| delafregering2     = [[Regeringen Anders Fogh Rasmussen I|Fogh Rasmussen I]], [[Regeringen Anders Fogh Rasmussen II|II]] og [[Regeringen Anders Fogh Rasmussen III|III]]\n",
            "\n",
            "| embede3            = [[Økonomiminister]]\n",
            "| embede_start3      = [[18. december]] [[1990]]\n",
            "| embede_slut3       = [[19. november]] [[1992]]\n",
            "| forgænger3         = [[Niels Helveg Petersen]]\n",
            "| efterfølger3       = [[Thor Pedersen]]\n",
            "| statsminister3     = [[Poul Schlüter]]\n",
            "| delafregering3     = [[Regeringen Poul Schlüter IV|Regeringen Schlüter IV]]\n",
            "\n",
            "| embede4            = [[Skatteminister]]\n",
            "| embede_start4      = [[10. september]] [[1987]]\n",
            "| embede_slut4       = [[19. november]] [[1992]]\n",
            "| forgænger4         = [[Isi Foighel]]\n",
            "| efterfølger4       = [[Peter Brixtofte]]\n",
            "| statsminister4     = [[Poul Schlüter]]\n",
            "| delafregering4     = [[Regeringen Poul Schlüter II|Regeringen Schlüter II]], [[Regeringen Poul Schlüter III|III]] og [[Regeringen Poul Schlüter IV|IV]]\n",
            "\n",
            "| embede5            = 8. formand for [[Venstre]]\n",
            "| næstformand5       = [[Lars Løkke Rasmussen]]\n",
            "| embede_start5      = [[18. marts]] [[1998]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnC0OylUMV6S",
        "colab_type": "code",
        "outputId": "99f00bc5-775f-46e4-a9b2-6ec76e02b24e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Download external package for extracting plain text from wiki xml.\n",
        "ext_pack = 'https://raw.githubusercontent.com/attardi/wikiextractor/master/WikiExtractor.py'\n",
        "import requests\n",
        "with open('WikiExtractor.py','wb') as f:\n",
        "  f.write(requests.get(ext_pack).content)\n",
        "import os \n",
        "if not os.path.isdir('da_files'):\n",
        "  os.mkdir('da_files')\n",
        "! python WikiExtractor.py --output da_files --bytes 10000000000 --quiet {unzipped_file}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘da_files’: File exists\n",
            "WARNING: Template errors in article 'Muse (gruppe)' (98064): title(1) recursion(0, 0, 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojS9SdNCNjpV",
        "colab_type": "code",
        "outputId": "1fc06a34-9e26-4f3a-ac63-b654cffacd4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# inspect files generated\n",
        "! ls -sh da_files/AA"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 366M\n",
            "366M wiki_00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DANBJ4vUSnX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct train and eval dataset iterating through the large file line by line.\n",
        "import os\n",
        "if not os.path.isdir('data'):\n",
        "  os.mkdir('data')\n",
        "efile = open('data/eval.txt','w')\n",
        "tfile = open('data/train.txt','w')\n",
        "import random\n",
        "p = 0.01\n",
        "for line in open('da_files/AA/wiki_00','r'):\n",
        "  if line.strip()=='':\n",
        "    continue\n",
        "  if random.random()>p:\n",
        "    tfile.write(line+'\\n')\n",
        "  else:\n",
        "    efile.write(line+'\\n')\n",
        "tfile.close()\n",
        "efile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqnWiHteSs-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Appropriation of the Transformers Tutorial on training a language model from scratch."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFi0jyw8TNIo",
        "colab_type": "code",
        "outputId": "f0b5e02c-007d-478c-dc37-729ef16fad2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# install the tokenizers library developed by the huggingface group.\n",
        "! pip install tokenizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/de/ec55e2d5a8720557b25100dd7dd4a63108a44b6b303978ce2587666931cf/tokenizers-0.6.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqzrNj6vTNvh",
        "colab_type": "code",
        "outputId": "35bb0216-7a2b-4ce1-9ebf-4d2b36cd44af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# set path to full dataset\n",
        "path2data = 'da_files/AA/wiki_00'\n",
        "import os\n",
        "os.path.isfile(path2data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn2QJaFuT8qC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Train tokenizers.\n",
        "# Initialize a tokenizer\n",
        "# Save files to disk\n",
        "# test tokenizer\n",
        "# test tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hr8vB9eTUzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define config file for the run_language_modelling.py script\n",
        "# define config file."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wysruwxhUJVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download script training \n",
        "import requests\n",
        "with open('run_language_modeling.py','w') as f:\n",
        "  f.write(requests.get('https://raw.githubusercontent.com/huggingface/transformers/master/examples/run_language_modeling.py').text)\n",
        "# install transformers package.\n",
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuHRTLttWat1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### UGLY HACK do not touch or look##### \n",
        "s = open('run_language_modeling.py').read()\n",
        "s = s.replace('''from transformers import (\\n    CONFIG_MAPPING,\\n    MODEL_WITH_LM_HEAD_MAPPING,\\n    WEIGHTS_NAME,\\n    AdamW,\\n    AutoConfig,\\n    AutoModelWithLMHead,\\n    AutoTokenizer,\\n    PreTrainedModel,\\n    PreTrainedTokenizer,\\n    get_linear_schedule_with_warmup,\\n)\\n'''\n",
        ",'''from transformers import (\\n   WEIGHTS_NAME,\\n    AdamW,\\n    AutoConfig,\\n    AutoModelWithLMHead,\\n    AutoTokenizer,\\n    PreTrainedModel,\\n    PreTrainedTokenizer,\\n    get_linear_schedule_with_warmup,\\n)\\n\n",
        "from collections import OrderedDict\n",
        "\n",
        "from transformers.configuration_albert import ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, AlbertConfig\n",
        "from transformers.configuration_bart import BART_PRETRAINED_CONFIG_ARCHIVE_MAP, BartConfig\n",
        "from transformers.configuration_bert import BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, BertConfig\n",
        "from transformers.configuration_camembert import CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, CamembertConfig\n",
        "from transformers.configuration_ctrl import CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP, CTRLConfig\n",
        "from transformers.configuration_distilbert import DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, DistilBertConfig\n",
        "from transformers.configuration_flaubert import FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, FlaubertConfig\n",
        "from transformers.configuration_gpt2 import GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP, GPT2Config\n",
        "from transformers.configuration_openai import OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP, OpenAIGPTConfig\n",
        "from transformers.configuration_roberta import ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP, RobertaConfig\n",
        "from transformers.configuration_t5 import T5_PRETRAINED_CONFIG_ARCHIVE_MAP, T5Config\n",
        "from transformers.configuration_transfo_xl import TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP, TransfoXLConfig\n",
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers.configuration_xlm import XLM_PRETRAINED_CONFIG_ARCHIVE_MAP, XLMConfig\n",
        "from transformers.configuration_xlm_roberta import XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP, XLMRobertaConfig\n",
        "from transformers.configuration_xlnet import XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP, XLNetConfig\n",
        "CONFIG_MAPPING = OrderedDict(\n",
        "    [\n",
        "        (\"t5\", T5Config,),\n",
        "        (\"distilbert\", DistilBertConfig,),\n",
        "        (\"albert\", AlbertConfig,),\n",
        "        (\"camembert\", CamembertConfig,),\n",
        "        (\"xlm-roberta\", XLMRobertaConfig,),\n",
        "        (\"bart\", BartConfig,),\n",
        "        (\"roberta\", RobertaConfig,),\n",
        "        (\"flaubert\", FlaubertConfig,),\n",
        "        (\"bert\", BertConfig,),\n",
        "        (\"openai-gpt\", OpenAIGPTConfig,),\n",
        "        (\"gpt2\", GPT2Config,),\n",
        "        (\"transfo-xl\", TransfoXLConfig,),\n",
        "        (\"xlnet\", XLNetConfig,),\n",
        "        (\"xlm\", XLMConfig,),\n",
        "        (\"ctrl\", CTRLConfig,),\n",
        "    ]\n",
        ")\n",
        "from transformers.modeling_auto import (\n",
        "        MODEL_MAPPING,\n",
        "        MODEL_FOR_PRETRAINING_MAPPING,\n",
        "        MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
        "        MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n",
        "        MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,\n",
        "        MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    )''').replace('model = AutoModelWithLMHead(config=config)',\n",
        "                  '''model = AutoModelWithLMHead.from_config(config=config)'''\n",
        "        \n",
        "    )\n",
        "\n",
        "with open('run_language_modeling2.py','w') as f:\n",
        "  f.write(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkPoICskVq96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define cmd to run the slightly altered script:\n",
        "# : run_language_modelling2.py\n",
        "# Start training."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}