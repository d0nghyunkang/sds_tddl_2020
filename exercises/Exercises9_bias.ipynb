{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercises9_bias.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5NckjMd1sKU",
        "colab_type": "text"
      },
      "source": [
        "# Exercise set 9: Bias detection and correction\n",
        "In this set you will practice: bias detection, correction, and mitigation.\n",
        "Again you will use the [Toxicity classification dataset](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) from the other exercises.\n",
        "Furthermore you should download the dataset provided by Kiritchenko & Mohammed 2018 [(data)](https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip) for testing biases in text classification systems \n",
        "\n",
        "\n",
        "The classifier we will test is based on the Universal Sentence Encoder that we used in [exercise set 8](https://github.com/ulfaslak/sds_tddl_2020/blob/master/exercises/week8_exercises_transferlearning.ipynb).\n",
        "\n",
        "\n",
        "## 9.0 Create function that takes in a train and test data and trains the transfer learning model from last weeks exercises. \n",
        "  - **Hint** initialize the hublayer outside the function, and use the `tf.keras.models.clone_model()` function to avoid downloading the layer everytime you reinitialize.\n",
        "\n",
        "## 9.1 Estimate Differential Bias \n",
        "Here we shall look at both individual classification bias and proportional classification bias.\n",
        "1. Train classifier on Toxicity dataset.\n",
        "2. Estimate Differential Biases for each of the minority populations. i.e. white black asian jewish etc columns. \n",
        "  - You need to set a threshold of how many percent of the annotators who aggreed on the Minority group (e.g. >0.5)\n",
        "  - Using the testset you should construct a confusion matrix for each minority.  \n",
        "  - Report the Accuracy, Precision, Recall and F1 score.\n",
        "3. Test the *\"Classify&Count\"* method for estimating proportion for the general popolation and then for the subpopulations of each minority group.\n",
        "  - See which groups have most error. \n",
        "  - Report Proportional classification accuracy using the pearsons product moment correlation (np.coercoef), and the root mean square deviation (RMSD). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzVH2_St2_-w",
        "colab_type": "code",
        "outputId": "1eb31a21-5ef9-46b8-a158-3e9227024116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# load dataset\n",
        "import pandas as pd\n",
        "path2tox_data = '/content/drive/My Drive/lm/toxic_train.csv'\n",
        "tox_df = pd.read_csv(path2tox_data)\n",
        "\n",
        "tox_df['label'] = (tox_df.target>0.5).astype(int)\n",
        "print(tox_df.shape)\n",
        "# subsample data to allow faster prototyping\n",
        "# df = df.sample(5000) # simple solution\n",
        "# stratified solution where we subsample from each meta data column to get a higher variance.\n",
        "strat_sample_cols = list(tox_df.columns[3:23])+['physical_disability',\n",
        "       'psychiatric_or_mental_illness', 'transgender', 'white']\n",
        "samples = []\n",
        "n = 500\n",
        "for col in strat_sample_cols:\n",
        "    binary = pd.DataFrame((tox_df[col]>0.5).astype(int))\n",
        "    samples+=[j for _,j in binary.groupby(col).apply(lambda x: x.sample(min(len(x),n//2))).index]\n",
        "idx = list(set(samples))\n",
        "df = tox_df.iloc[idx].copy()\n",
        "\n",
        "sample = df.groupby('label').apply(lambda x: x.sample(500))\n",
        "sample_texts = sample.comment_text.values\n",
        "print(df.shape,tox_df.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1804874, 46)\n",
            "(11138, 46) (1804874, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdMarCnhBZI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solution 9.0\n",
        "## Initialize hublayer outside of train_function.\n",
        "# Make basemodel to avoid downloading everytime.\n",
        "# Add USE layer to model  \n",
        "# Define train_transfer_use function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ErgqBNX52V",
        "colab_type": "text"
      },
      "source": [
        "## 9.2 Correct the Bias using \"Ideal Method\" (i.e. that labelled and unlabelled are drawn randomly from the same distribution).\n",
        "Here we need to split the data into 3 sets: train, eval, and test. Because we need a decent amount of samples in the evaluation set, we need to use more of the \"expensive\" labelled data. Luckily we have a large toxicity dataset referenced under `tox_df` if you use my loading cell. \n",
        "  1. Train a new classifier on train data. \n",
        "  2. Estimate aggregrate confusion matrix, and then for individual minotiry groups on the evaluation data.\n",
        "  3. Apply classifier to the rest of the data (i.e. the test set) and correct the predictions as described in the lecture and Hopkins and King (2010:235). $P(D)=\\frac{T P-F P}{T P+F P} * \\hat{P}+\\frac{F N}{F N+T N} * \\hat{N}$ where P(D) is the probability of a document category. $\\hat{P}$ and $\\hat{N}$ is positive predictions and negative predictions.\n",
        "  4. Report the proportional classification error as above, and comment on the improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx9k62uRX_pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Solution 9.2 Bias Correction."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK1tJx_iYCaV",
        "colab_type": "text"
      },
      "source": [
        "## 9.3. Compare to the direct estimation method suggested by Hopkins & King 2010 and Jerzak, King, and Strezhnev 2020. \n",
        "Since the methods proposed in the above papers, is based on other feature representation schemes, it is not entirely meaningful as a comparison to the *Classify-and-count\" method. Instead we use the same feature representation as the classifier, i.e. the Universal Sentence Encoder, and estimate the equation.\n",
        "\n",
        "As referenced from the text by Hopkins and King:\n",
        "\n",
        "\"*think of P(D) as the unknown “regression coefficients”  $\\beta$, P(S|D) as the “explanatory variables” matrix X, and P(S) as the “dependentvariable” Y.*\"\n",
        "\n",
        "Where P(D) is the probability of a document category (observed as no. of Positive Labels in the training set). P(S) is the probability of the document feature representation (again observed in the training data). P(S|D) is the probability of the Document Representation given Document Category D, which should be estimated using the standard linear regression calculations. I.e. solve for $\\beta$: \n",
        "\n",
        "$\\beta=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} y$\n",
        "\n",
        "So what you need to do is the following:\n",
        "1. Encode the texts from the data to define your x_train and x_test.\n",
        "2. Estimate P(S|D) i.e. $\\beta$. **hint**: use the `np.linalg.inv` function, the `.T` and the `.dot` function.\n",
        "3. Estimate Proportions of each document in the test data by taking the dot product between P(S) and $\\beta$. \n",
        "4. Aggregate to estimate proportions.\n",
        "  - Report overall and individual minority proportions estimated compared to the test. \n",
        "  - Report measures for Proporational classification (rmsd and correlation).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT8_IKuQ4-9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Solution 9.3: Direct estimation\n",
        "## Encode text using the Universal Feature Encoder (hint: use the \"hublayer-only\"-model you clone when training your classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxtDAfYcI0tn",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 9.4: Bias detection using critical test cases\n",
        "This is about making the bias of the model visible, by questioning specific dimensions (e.g. race, gender, age).. Method is demonstrated in the paper: [\"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems\"](https://arxiv.org/abs/1805.04508) by Kiritchenko & Mohammad 2018:. [data](https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip)\n",
        "You will use this data to test a pretrained system. Then you will practice curate your own specific \"bias-detection-scheme\", by combining the methods you have learned so far: keyword exploration using the [King, Lam and Robert 2017](https://gking.harvard.edu/publications/computer-assisted-keyword-and-document-set-discovery-fromunstructured-text)'s method - exercise set for week 7, universal dependence parsing with stanfordnlp now known as the [`stanza`](https://stanfordnlp.github.io/stanza/) package. \n",
        "You will use this to create what is known as data augmentation scheme which can be used both for \"bias detection\" as well as \"bias mitigation\". \n",
        "\n",
        "First are some helper functions for preparing the data and models you should use. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6OGtvnO-0W",
        "colab_type": "text"
      },
      "source": [
        "## Loading bias detection data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMHLO7sDLrF_",
        "colab_type": "code",
        "outputId": "4fd3715a-d811-48fe-b862-2ff4f45e5aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "## Download the equity evaluation corpus\n",
        "### linux commandline version\n",
        "link = 'https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip'\n",
        "! wget {link}\n",
        "path = link.split('/')[-1]\n",
        "dir_to_extr = 'bias_dataset'\n",
        "import os\n",
        "if not os.path.isdir(dir_to_extr):\n",
        "  os.mkdir(dir_to_extr)\n",
        "\n",
        "#! unzip {path} -d {dir_to_extr]\n",
        "os.system('unzip %s -d %s'%(path,dir_to_extr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-02 09:24:29--  https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1669592 (1.6M) [application/zip]\n",
            "Saving to: ‘Equity-Evaluation-Corpus.zip.7’\n",
            "\n",
            "Equity-Evaluation-C 100%[===================>]   1.59M  1.44MB/s    in 1.1s    \n",
            "\n",
            "2020-04-02 09:24:30 (1.44 MB/s) - ‘Equity-Evaluation-Corpus.zip.7’ saved [1669592/1669592]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-xSLhkuMPmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Download the equity evaluation corpus\n",
        "### Python version\n",
        "import requests\n",
        "link = 'https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip'\n",
        "session = requests.session()\n",
        "session.headers = '' # for some reason they block explicit python requests.\n",
        "response = session.get(link)\n",
        "with open('Equity-Evaluation-Corpus.zip','wb') as f:\n",
        "    f.write(response.content)\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('Equity-Evaluation-Corpus.zip', 'r')\n",
        "dir_to_extr = 'bias_dataset'\n",
        "import os\n",
        "if not os.path.isdir(dir_to_extr):\n",
        "    os.mkdir(dir_to_extr)\n",
        "zip_ref.extractall(dir_to_extr)\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfptWhNKjxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "dir_to_extr = 'bias_dataset/Equity-Evaluation-Corpus'\n",
        "bias_df = pd.read_csv(dir_to_extr+'/Equity-Evaluation-Corpus.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS0AlsLYPHLL",
        "colab_type": "text"
      },
      "source": [
        "## Loading the deepmoji model. \n",
        "As transfer learning is about using pretrained models, one has to be flexible in relation to the choice of deep learning framework. A working model of the DeepMoji is implemented by the [HuggingFace team](https://huggingface.co/welcome) under the name [TorchMoji](https://github.com/huggingface/torchMoji), which is basically a Pytorch implementation adapted from the python 2.7 Keras implementation made by Bjarke Felbo (https://github.com/bfelbo/DeepMoji/tree/master/deepmoji). \n",
        "\n",
        "The script will do the following:\n",
        "- Clone the github repo. \n",
        "- Download the model weights \n",
        "- install dependencies hereunder the `emoji` python package.\n",
        "- Add the torchmoji to the python syspath for easy import. \n",
        "- Load the tokenizer that deepmoji depends on.\n",
        "- Load the model.\n",
        "- Define a helper function for translating \"literal-emojies\" to unicode emojies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihZRfDW-P_7J",
        "colab_type": "code",
        "outputId": "eaec1277-1305-4f24-ceac-704a326b3be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "source": [
        "## clone the repository\n",
        "! git clone https://github.com/huggingface/torchMoji.git\n",
        "## download the pretrained model's weights using their script\n",
        "import os\n",
        "cwd = os.getcwd()\n",
        "os.chdir('torchMoji')\n",
        "! python scripts/download_weights.py\n",
        "\n",
        "import os\n",
        "#os.chdir('torchMoji')\n",
        "# navigate to the torchmoji folder\n",
        "## install dependencies\n",
        "#! pip install -e \n",
        "! pip install emoji"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'torchMoji'...\n",
            "remote: Enumerating objects: 143, done.\u001b[K\n",
            "remote: Total 143 (delta 0), reused 0 (delta 0), pack-reused 143\u001b[K\n",
            "Receiving objects: 100% (143/143), 2.41 MiB | 4.19 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n",
            "About to download the pretrained weights file from https://www.dropbox.com/s/q8lax9ary32c7t9/pytorch_model.bin?dl=0#\n",
            "The size of the file is roughly 85MB. Continue? [y/n]\n",
            "y\n",
            "Downloading...\n",
            "Running system call: wget https://www.dropbox.com/s/q8lax9ary32c7t9/pytorch_model.bin?dl=0# -O /content/torchMoji/model/pytorch_model.bin\n",
            "--2020-04-02 12:00:35--  https://www.dropbox.com/s/q8lax9ary32c7t9/pytorch_model.bin?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/q8lax9ary32c7t9/pytorch_model.bin [following]\n",
            "--2020-04-02 12:00:35--  https://www.dropbox.com/s/raw/q8lax9ary32c7t9/pytorch_model.bin\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucce3d2fc56ffb8b8658d2ead54d.dl.dropboxusercontent.com/cd/0/inline/A1H5v686UPLl1KxUKE7c92ZBoa6GRXQc6HAqNirW9bNX8-bi8K6Rt27TkgN7x5VQb8CGLldZSqqch2Q1hYp6hZy3W_JFkZuOtGFy91TSSS0JbA/file# [following]\n",
            "--2020-04-02 12:00:36--  https://ucce3d2fc56ffb8b8658d2ead54d.dl.dropboxusercontent.com/cd/0/inline/A1H5v686UPLl1KxUKE7c92ZBoa6GRXQc6HAqNirW9bNX8-bi8K6Rt27TkgN7x5VQb8CGLldZSqqch2Q1hYp6hZy3W_JFkZuOtGFy91TSSS0JbA/file\n",
            "Resolving ucce3d2fc56ffb8b8658d2ead54d.dl.dropboxusercontent.com (ucce3d2fc56ffb8b8658d2ead54d.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to ucce3d2fc56ffb8b8658d2ead54d.dl.dropboxusercontent.com (ucce3d2fc56ffb8b8658d2ead54d.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/A1EoyZ2Hf7VVAJukCvxYTRhz5BBV_aEClfKytEPNfcGJVul7N-uh2t6FDSVohcH1S9yuSKHvj1rKLJSzj8J8Wxuxu32VZFRcZ6j6z7uuyqdq2GnWlZHEVcS2QPaNNxxq_f-LTIqBCRr6hv2E59HwYaPkQN8gSaoz0Gj8h5oMqnYLW7HK8TQ__XwN-WKBs8rh01SLTVYGhmhmPGrx_Va5YxXYkyp_NX5s9aPoomKnoH4salYhOfulNtWuZROSckEj2MjgWtuxAC28ZwozTUx7749kNsTAN0DbiigFk3FIAyyP6SHdGHY-42tJtXe4dyRM-OoTMJgravE6ZtDYJBx4GcLr/file [following]\n",
            "--2020-04-02 12:00:37--  https://ucce3d2fc56ffb8b8658d2ead54d.dl.dropboxusercontent.com/cd/0/inline2/A1EoyZ2Hf7VVAJukCvxYTRhz5BBV_aEClfKytEPNfcGJVul7N-uh2t6FDSVohcH1S9yuSKHvj1rKLJSzj8J8Wxuxu32VZFRcZ6j6z7uuyqdq2GnWlZHEVcS2QPaNNxxq_f-LTIqBCRr6hv2E59HwYaPkQN8gSaoz0Gj8h5oMqnYLW7HK8TQ__XwN-WKBs8rh01SLTVYGhmhmPGrx_Va5YxXYkyp_NX5s9aPoomKnoH4salYhOfulNtWuZROSckEj2MjgWtuxAC28ZwozTUx7749kNsTAN0DbiigFk3FIAyyP6SHdGHY-42tJtXe4dyRM-OoTMJgravE6ZtDYJBx4GcLr/file\n",
            "Reusing existing connection to ucce3d2fc56ffb8b8658d2ead54d.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89616062 (85M) [application/octet-stream]\n",
            "Saving to: ‘/content/torchMoji/model/pytorch_model.bin’\n",
            "\n",
            "/content/torchMoji/ 100%[===================>]  85.46M  28.9MB/s    in 3.0s    \n",
            "\n",
            "2020-04-02 12:00:40 (28.9 MB/s) - ‘/content/torchMoji/model/pytorch_model.bin’ saved [89616062/89616062]\n",
            "\n",
            "Downloaded weights to model/pytorch_model.bin\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=41e6d3cbf29886aa27a1091407abc59dbb9e91807ddacfde70d02f620c49b986\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeoUZQbCQuNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add to sys.path\n",
        "import sys\n",
        "base_path = '' # change if you have downloaded folder elsewhere.\n",
        "base_path = 'torchMoji/' ## path to the torchmoji directory\n",
        "sys.path.insert(0, base_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGKnhasxQf0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
        "# load the deepmoji encoder that transforms text to emojies.\n",
        "from torchmoji.model_def import torchmoji_emojis\n",
        "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
        "import json,csv, numpy as np\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "\n",
        "## set the max context length\n",
        "max_token = 30 ## This will not work for longer texts,\n",
        "################# here you should consider splitting each text into smaller segments.\n",
        "\n",
        "# Load vocab (i.e. the index of each word in the vector representation)\n",
        "with open(VOCAB_PATH, 'r') as f:\n",
        "    vocabulary = json.load(f)\n",
        "\n",
        "# initialize tokenizer\n",
        "sentence_tokenizer = SentenceTokenizer(vocabulary, max_token)\n",
        "# load model\n",
        "model = torchmoji_emojis(PRETRAINED_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73W2jK_CSrd4",
        "colab_type": "text"
      },
      "source": [
        "### Load emoji translater to map output dimensions of the DeepMoji to unicode Emojies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xu0GP3NRKS9",
        "colab_type": "code",
        "outputId": "44ff95a8-bcc1-4c60-ebdd-416a69b94808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Change working directory back to normal. \n",
        "os.chdir(cwd)\n",
        "with open(base_path+'data/emoji_codes.json') as f:\n",
        "    emoji_desc = json.load(f)\n",
        "print(list(emoji_desc.items())[0:10])\n",
        "import emoji\n",
        "def translate_emoji(emoji_descr):\n",
        "    if emoji_descr in emoji.unicode_codes.EMOJI_ALIAS_UNICODE:\n",
        "        return emoji.unicode_codes.EMOJI_ALIAS_UNICODE[emoji_descr]\n",
        "    if emoji_descr in emoji.unicode_codes.EMOJI_UNICODE:\n",
        "        return emoji.unicode_codes.EMOJI_UNICODE[emoji_descr]\n",
        "    return emoji_descr\n",
        "to_emoji = [translate_emoji(desc) for i,desc in sorted(emoji_desc.items(),key=lambda x: int(x[0]))]\n",
        "to_emoji_desc = [desc for i,desc in sorted(emoji_desc.items(),key=lambda x: int(x[0]))]\n",
        "## index \n",
        "to_emoji[0],to_emoji_desc[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('0', ':joy:'), ('1', ':unamused:'), ('2', ':weary:'), ('3', ':sob:'), ('4', ':heart_eyes:'), ('5', ':pensive:'), ('6', ':ok_hand:'), ('7', ':blush:'), ('8', ':heart:'), ('9', ':smirk:')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('😂', ':joy:')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBdeuaNBS9z0",
        "colab_type": "text"
      },
      "source": [
        "### Now we are ready to encode the text as emojis using the pretrained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NObH7Zn3TanJ",
        "colab_type": "text"
      },
      "source": [
        "## 9.5.1: Investigate the bias of the [DeepMoji](https://deepmoji.mit.edu/) classifier from the paper [\"Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm\"](https://arxiv.org/pdf/1708.00524.pdf) using the Kiritchenko & Mohammed 2018 [dataset](https://saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip)\n",
        "\n",
        "The dataset contains identical sentences changing only the name of the person being referenced. \n",
        "\n",
        "Dataset used is referenced under the `bias_df`.\n",
        "\n",
        "|    | ID                    | Sentence                | Template                               | Person   | Gender   | Race             | Emotion   | Emotion word   |\n",
        "|---:|:----------------------|:------------------------|:---------------------------------------|:---------|:---------|:-----------------|:----------|:---------------|\n",
        "|  0 | 2018-En-mystery-05498 | Alonzo feels angry.     | <person subject> feels <emotion word>. | Alonzo   | male     | African-American | anger     | angry          |\n",
        "|  1 | 2018-En-mystery-11722 | Alonzo feels furious.   | <person subject> feels <emotion word>. | Alonzo   | male     | African-American | anger     | furious        |\n",
        "|  2 | 2018-En-mystery-11364 | Alonzo feels irritated. | <person subject> feels <emotion word>. | Alonzo   | male     | African-American | anger     | irritated      |\n",
        "\n",
        "\n",
        "The deepmoji model is referenced under as `model`.\n",
        "\n",
        "And the tokenizer is referenced as `sentence_tokenizer`.\n",
        "\n",
        "You should now do the following:\n",
        "\n",
        "1. Tokenization. Tokenize the documetns in the `bias_dataset`.\n",
        "  - Use the `sentence_tokenizer` defined above to tokenize the documents.\n",
        "\n",
        "  - see example in the torchmoji examples [e.g.](https://github.com/huggingface/torchMoji/blob/master/examples/encode_texts.py) folder for help.\n",
        "\n",
        "  - Inspect the tokenized documents to see the format.\n",
        "  - Try to convert them back using <code>vocabulary</code> variable defined earlier. **- Hint this means reversing the vocabulary dictionary.**\n",
        "2. DeepMoji encoding\n",
        "  - Encode the tokenized sentences and wrap it in a function.\n",
        "  - Hint: Do a forward pass of the model on the tokenized data. Check [here](https://github.com/huggingface/torchMoji/blob/master/examples/encode_texts.py) for help \n",
        "\n",
        "  - For larger datasets and with longer sentences encoding is problematic if not done in batches. \n",
        "  - Write a for loop that takes only 256 tokenized documents at a time and concatenate them to a dataframe in the end.\n",
        "  - Use the <code>to_emoji list </code> as columns in the dataframe\n",
        "3. Join DeepMoji Encoding with the `bias_df`.\n",
        "  -  Join the output of Deepmoji with the bias dataframe columns (Race, Gender and Emotion)\n",
        "  - Make sure Race count and Gender counts are equal after join.\n",
        "4. Investigate if there are significant differences in relation to **Race** (Race column).\n",
        "  - See which types of emojies are most changed by a change in race or gender.\n",
        "  - See which *Emotions* (Emotion column) have largest difference in encoding in relation to different races. \n",
        "    - I.e. Groupby Emotion and Race and calculate absolute difference in emoji encoding. \n",
        "    - hint: first groupby emotion and race, calculate mean, then diff, then abs and then sum.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nymvt6WNTZPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Solution 9.5.1 Tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR9icwxkb7DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Solution 9.5.2 DeepMoji Encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqyuyK0SVq_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solution 9.5.3 Join with bias_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGfWvyjofm_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solution 9.5.4 Analyzing how Gender and Race alone changes the DeepMoji encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6IPZ5CpTh91",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 9.6.1: Create your own test case by substituting minority identifiers in the toxicity dataset.\n",
        "\n",
        "The key to the bias test was substituting using the same sentences but with different subjects in the sentence, (Alonzo, Alan, He, she) etc. We can construct a similar dataset by *Augmenting* the toxicity dataset (referenced as `df`).\n",
        "- First we create our minority identifiers that we want to substitute / remove. Could have been done iteratively using Exploration methods like (word similarity search or active learning style keyword discovery from most predictive features). \n",
        "  - Instead we will pick most predictive phrases from each minority category in the data. \n",
        "    1. Because we will use it for our data augmentation scheme we want slightly more information than just words. We therefore tokenize and process documents using the standfordnlp package. ```! pip install stanza\n",
        "import stanza\n",
        "stanza.download('en') # download English model\n",
        "nlp = stanza.Pipeline('en') # initialize English neural pipeline```\n",
        "    2. Aply the nlp pipeline to all documents.\n",
        "    3. Extract word and wordtype pairs from all documents. i.e. a document will look like this: `[(w, wtyp), (w1,wtyp) ... (wi,wtyp)]`. Remember to lowercase.\n",
        "    4. Use the `bow_to_sparse` helper function to create an index using the most prevalent word,wordtype pairs and transform the documents to bows. The function returns `sparse_matrix,index`, which denotes a matrix of word_pair counts, and the corresponding index of each wordpair. \n",
        "    5. Train a classifier (logistic regression) for each minority column.\n",
        "    6. Extract most predictive features (i.e. `.coef_`). \n",
        "    7. Go through the phrases and pick the most useful (at least 10).\n",
        "  - Do the above for at least 3 different minorities.  \n",
        "\n",
        "- Now we want to Remove the minority identifiers and see how our model does.\n",
        "  - Write a function that takes a list of identifiers and replaces them with a pattern. \n",
        "    - `def change_identifier(identifiers,replace_pattern=''):` \n",
        "  - Apply the function `change_identifier` function to the texts to create your *\"augmented\"* dataset.\n",
        "  - Train two models:\n",
        "    - One on all the texts. \n",
        "    - One with on the *augmented* the texts where all minority identifiers are \n",
        "  - Report the differences in predictions. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TnBRb8Qnfp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install stanza\n",
        "import stanza\n",
        "stanza.download('en') # download English model\n",
        "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8hE5sKypBUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SOlution 9.6.1 APply stanza pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR9Th0FspG_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SOlution 9.6.2 Extract word pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6am2JnhqpHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Helper function bow_to_sparse.\n",
        "import scipy.sparse as sp\n",
        "from collections import Counter\n",
        "def bow_to_sparse(docs,max_vocab_size=32768):\n",
        "    c = Counter()\n",
        "    bows = []\n",
        "    for doc in docs:\n",
        "      bow = Counter(doc)\n",
        "      bows.append(bow)\n",
        "      c.update(bow)\n",
        "    w2i = {w:num for num,(w,count) in enumerate(c.most_common()[0:max_vocab_size])}\n",
        "\n",
        "    idx = sorted(w2i,key=lambda x: w2i[x])\n",
        "    vocab_size = len(idx)\n",
        "    X = sp.dok_matrix((len(docs),vocab_size), dtype=np.int32)\n",
        "    for num in range(len(docs)):\n",
        "        bow = bows[num]\n",
        "        for w,count in bow.items():\n",
        "            try:\n",
        "              wi = w2i[w]\n",
        "            except:\n",
        "              continue\n",
        "            X[num,wi]=count\n",
        "    print(X.shape)\n",
        "    X = X.tocsr()\n",
        "    print(X.shape)\n",
        "    return X,idx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lAicfle5CaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solution 9.6.3 Get minority identifiers using a variation over the King, Roberts and Lam 2017 method."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DYhKVq8qrAt",
        "colab_type": "text"
      },
      "source": [
        "# 9.7: Data augmentation to mitigate bias. \n",
        "Here you will create synthetic data to *mitigate* the minority biases by training the model on data where the different minority identifiers are used interchangebly.\n",
        "\n",
        "The strategy is to create new synthetic data by substituting an identifier from e.g. \"black\" identifiers with \"white\" identifiers, and we want to only substitute columns with the same Wordtyp. \n",
        "\n",
        "- Define a function that takes Two Sets of identifiers and substitutes identifiers from each set: \n",
        "  1. Makes a copy of the training BoWs (i.e. `.copy()` function). Runs through each identifer in set 1, locates the rows where it is active and makes a copy of these rows. \n",
        "  2. Sets identifer column to 0.\n",
        "  3. For each identifier in set 2 that match the same wordtype, make a copy of the rows. Set identifier column to 1 and append to list. \n",
        "  4. Concatenate these sparsematrices to one sparse matrix using the `scipy.sparse.vstack` function.\n",
        "\n",
        "- Apply the function to create extra synthethic data using all identifier sets.\n",
        "- Train a classifier using both the old and new synthetic data and investigate its differential bias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZJMuJZR43CR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solution 9.7.2 Dataaugmentation\n",
        "# Substitution function\n",
        "\n",
        "# Create synthetic dataset\n",
        "\n",
        "# Train Classifier."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sPSi1kl4xF0",
        "colab_type": "text"
      },
      "source": [
        "## Extra\n",
        "- Implement a simple semi-supervised learning classifier. \n",
        "\n",
        "- Compute the lift curve. What does the graph tell you about the Saturation of your classifier?\n",
        "\n",
        "\n"
      ]
    }
  ]
}